{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abd69e3c-f346-4e7a-a97f-32673c6c67b3",
   "metadata": {},
   "source": [
    "### First, we import the relevant libraries for performing the tasks we require"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a94a28e-2514-46ad-86a6-2c3b3e7b9ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nequip\n",
      "  Downloading nequip-0.6.1-py3-none-any.whl.metadata (786 bytes)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from nequip) (1.26.4)\n",
      "Collecting ase (from nequip)\n",
      "  Downloading ase-3.23.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from nequip) (4.66.4)\n",
      "Collecting e3nn<0.6.0,>=0.4.4 (from nequip)\n",
      "  Downloading e3nn-0.5.1-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: pyyaml in /opt/anaconda3/lib/python3.12/site-packages (from nequip) (6.0.1)\n",
      "Collecting torch-runstats>=0.2.0 (from nequip)\n",
      "  Downloading torch_runstats-0.2.0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting torch-ema>=0.3.0 (from nequip)\n",
      "  Downloading torch_ema-0.3-py3-none-any.whl.metadata (415 bytes)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.12/site-packages (from e3nn<0.6.0,>=0.4.4->nequip) (1.12)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (from e3nn<0.6.0,>=0.4.4->nequip) (1.13.1)\n",
      "Requirement already satisfied: torch>=1.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from e3nn<0.6.0,>=0.4.4->nequip) (2.3.1)\n",
      "Collecting opt-einsum-fx>=0.1.4 (from e3nn<0.6.0,>=0.4.4->nequip)\n",
      "  Downloading opt_einsum_fx-0.1.4-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: matplotlib>=3.3.4 in /opt/anaconda3/lib/python3.12/site-packages (from ase->nequip) (3.8.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=3.3.4->ase->nequip) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=3.3.4->ase->nequip) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=3.3.4->ase->nequip) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=3.3.4->ase->nequip) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=3.3.4->ase->nequip) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=3.3.4->ase->nequip) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=3.3.4->ase->nequip) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=3.3.4->ase->nequip) (2.9.0.post0)\n",
      "Collecting opt-einsum (from opt-einsum-fx>=0.1.4->e3nn<0.6.0,>=0.4.4->nequip)\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip) (4.11.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip) (2024.3.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.12/site-packages (from sympy->e3nn<0.6.0,>=0.4.4->nequip) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.4->ase->nequip) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip) (2.1.3)\n",
      "Downloading nequip-0.6.1-py3-none-any.whl (175 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.4/175.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading e3nn-0.5.1-py3-none-any.whl (118 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch_ema-0.3-py3-none-any.whl (5.5 kB)\n",
      "Downloading torch_runstats-0.2.0-py3-none-any.whl (8.1 kB)\n",
      "Downloading ase-3.23.0-py3-none-any.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum_fx-0.1.4-py3-none-any.whl (13 kB)\n",
      "Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch-runstats, opt-einsum, torch-ema, opt-einsum-fx, ase, e3nn, nequip\n",
      "Successfully installed ase-3.23.0 e3nn-0.5.1 nequip-0.6.1 opt-einsum-3.3.0 opt-einsum-fx-0.1.4 torch-ema-0.3 torch-runstats-0.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.3.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "!pip install nequip\n",
    "\n",
    "### Library for training machine learning potentials\n",
    "import nequip\n",
    "\n",
    "### Library for data storage\n",
    "import pickle\n",
    "\n",
    "### Library of mathematical functions\n",
    "import numpy as np\n",
    "\n",
    "### Library for plotting results\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debe0e00-0a15-4d83-bb94-9ea6a387e4c0",
   "metadata": {},
   "source": [
    "### Next, we can read the data from the pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a81f1fce-bdd4-499e-92c5-a4c276725025",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This opens the data file as the variable 'f' and reads the contents into each variable atoms, energies...\n",
    "with open(\"atoms_energies_forces_stresses.pkl\",\"rb\") as f:\n",
    "    structures, energies, forces, stresses = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05226f9d-4965-40f9-9167-2b422a8d2e06",
   "metadata": {},
   "source": [
    "### The variables atoms, energies, forces and stresses are lists which means that it is just a list of things, such as numbers, matricies or even python objects. Lets take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de2f13c7-c311-42b2-8846-073fc4ce3df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first energy = -97.4161547274548 eV\n",
      "\n",
      "The first force = [[ 5.69909872e-02 -1.76185541e-03 -2.23835194e-02]\n",
      " [-5.69916568e-02 -1.76180701e-03  2.23841691e-02]\n",
      " [-5.69907920e-02  1.76206263e-03 -2.23834784e-02]\n",
      " [ 5.69923567e-02  1.76119312e-03  2.23841928e-02]\n",
      " [-5.69917025e-02  1.76141469e-03  2.23837663e-02]\n",
      " [ 5.69925485e-02  1.76140587e-03 -2.23842864e-02]\n",
      " [ 5.69906477e-02 -1.76215856e-03  2.23832952e-02]\n",
      " [-5.69934419e-02 -1.76046307e-03 -2.23846119e-02]\n",
      " [ 1.17626792e-08  2.33927185e-07  6.50083823e-03]\n",
      " [-5.51220743e-09 -2.86324382e-07 -6.50026872e-03]\n",
      " [ 1.85004604e-08 -9.71142470e-08 -6.50115062e-03]\n",
      " [-8.62011135e-08  1.41895065e-07  6.50059329e-03]] in units of eV/angstrom\n",
      "\n",
      "The first stress= [[-1.51929838e+02  8.33082365e-06  1.13971169e-05]\n",
      " [ 8.21279775e-06 -1.48890261e+02 -1.12902036e-05]\n",
      " [ 1.12940485e-05 -1.13501092e-05 -1.52194604e+02]] in units of kilobars\n"
     ]
    }
   ],
   "source": [
    "print(\"The first energy =\", energies[0], \"eV\")\n",
    "print()\n",
    "print(\"The first force =\", forces[0], \"in units of eV/angstrom\")\n",
    "print()\n",
    "print(\"The first stress=\", stresses[0],\"in units of kilobars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a64a192e-b1d2-4854-a684-1e9c4825cdd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['/opt/anaconda3/bin/python', '-m', 'ase', 'g...>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Lets also take a look at the 25th structure in our list of structures \n",
    "### Remember! Python indicies start at 0, so the first structure in our list is actually structures[0]. This means\n",
    "### structures[24] is actually the 25th structure\n",
    "from ase.visualize import view\n",
    "view(structures[24])\n",
    "\n",
    "### You should be able to right click and drag in the pop-up to see the atoms and the unit cell. \n",
    "### If you go to the toolbar in the pop-up and click view->repeat and increase the numbers and go back to the pop-up,\n",
    "### You will see how the crystal structure gets formed by simply duplicating the unit cell to the left, right, up, down, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e62f0e6-85f4-45b8-ae21-0f93a0aa05a9",
   "metadata": {},
   "source": [
    "### If you count the number of atoms in the unit cell, you should notice that there are 12. You should also notice that there are 12 vectors for the forces that we printed before, like $$[[x_1, y_1, z_1], [x_2, y_2, z_2]...[x_{12}, y_{12}, z_{12}]]$$ These are the forces on each atom in the unit cell in this configuration. You can see that the forces are small, but not zero. This means the atoms want to move and are not stable in this configuration\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000b0e9a-0a59-49c2-96a7-ec93fef38f7f",
   "metadata": {},
   "source": [
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a851c546-1df2-401a-b04d-2fb8f1750d88",
   "metadata": {},
   "source": [
    "### Each structure at index i (such as structures[i]) have corresponding forces at index i (forces[i]) as well as energies and stresses. We won't worry about stresses for now and focus on the energies and forces <br>  Lets visualize the distribution of energies now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9267553e-4b91-48d4-a1db-2c1d2204b009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### First lets make a list of numbers that go from 0 to the length of energies. This is so that we can make a scatter\n",
    "### plot of the energies for each structure at index x\n",
    "x = np.arange(0, len(energies))\n",
    "plt.plot(x, energies)\n",
    "\n",
    "### This is so that the plot displays in the jupyter notebook. In actual python script, we would use plt.show()\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7377e2-36d5-4a36-860f-f6ddbb277f5b",
   "metadata": {},
   "source": [
    "### We have a thousand structures in our list, so the x-axis extends to 1000. Keep in mind that values on the y-axis are in eV (electron volts) <br> <br> Lets also plot the largest force vector on the atoms in the structure for each structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ee3648d-a930-419c-90eb-374440585daa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x15feb22d0>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "largest_force_magnitudes = []\n",
    "for i in range(len(forces)):\n",
    "    ### Here we save the magnitude of the force for each force on each atom in the unit cell\n",
    "    force_magnitudes = []\n",
    "    for j in range(len(forces[i])):\n",
    "        ### We use the numpy library to calculate the length of the force on atom j, which is referred to as the magnitude\n",
    "        force_magnitude = np.linalg.norm(forces[i][j])\n",
    "        force_magnitudes.append(force_magnitude)\n",
    "\n",
    "    ### Finally, we use the numpy library to find the largest force magnitude for this structure\n",
    "    largest_force = np.amax(force_magnitudes)\n",
    "    largest_force_magnitudes.append(largest_force)\n",
    "\n",
    "\n",
    "### Next, we plot each force magnitude with respect to the index of the structure\n",
    "x = np.arange(0, len(forces))\n",
    "plt.plot(x, largest_force_magnitudes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bb6ed0-9d28-4479-bbd9-c217caeec20d",
   "metadata": {},
   "source": [
    "### We can see that for the early structures (the ones near x=0), even the largest force is quite small. However, the structures later on have a much larger force. This is because these results come from a molecular dynamics simulation, which means the atoms are moving around constantly and subsequently being pushed constantly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9eb736-a2a0-4ff6-b513-96765ddf85f5",
   "metadata": {},
   "source": [
    "### Now that we understand the format of the data, lets create a file that NEQUIP can read for training. <br>\n",
    "### We will be making an extxyz file. This means that energies and forces will be written for each configuration of atoms. Lets do a simple test to verify it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db6bde09-889f-4082-b594-c7ca5c7f4ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-92.87576295593863 = -92.87576295593863\n",
      "\n",
      "[[ 3.33797564  1.62464736 -3.84386113]\n",
      " [-3.6782352  -0.14456652 -1.00140032]\n",
      " [-2.09448968  2.85214464 -0.57546884]\n",
      " [ 1.98640845 -2.08304851 -0.90779581]\n",
      " [ 0.53734864 -0.38469393 -1.69818315]\n",
      " [ 1.66344134  2.51305205  2.59690119]\n",
      " [ 2.00028782 -1.18841462  1.89392922]\n",
      " [ 1.04344342  1.11123829  1.88026772]\n",
      " [ 0.18837934  0.51097662 -3.16485836]\n",
      " [-2.9094413  -4.04642925  1.61256092]\n",
      " [-2.95077166 -5.85190149  0.55686183]\n",
      " [ 0.87546735  5.08684284  2.65092949]] = [[ 3.33797564  1.62464736 -3.84386113]\n",
      " [-3.6782352  -0.14456652 -1.00140032]\n",
      " [-2.09448968  2.85214464 -0.57546884]\n",
      " [ 1.98640845 -2.08304851 -0.90779581]\n",
      " [ 0.53734864 -0.38469393 -1.69818315]\n",
      " [ 1.66344134  2.51305205  2.59690119]\n",
      " [ 2.00028782 -1.18841462  1.89392922]\n",
      " [ 1.04344342  1.11123829  1.88026772]\n",
      " [ 0.18837934  0.51097662 -3.16485836]\n",
      " [-2.9094413  -4.04642925  1.61256092]\n",
      " [-2.95077166 -5.85190149  0.55686183]\n",
      " [ 0.87546735  5.08684284  2.65092949]]\n"
     ]
    }
   ],
   "source": [
    "### Lets choose structure 11 to test on\n",
    "teststructure = structures[10].copy()\n",
    "\n",
    "### Here we import this \"calculator\" to simply store the values of energy and forces\n",
    "from ase.calculators.singlepoint import SinglePointCalculator\n",
    "\n",
    "### We define the \"calc\" of this structure to just be a function that returns either the energies or the forces\n",
    "teststructure.calc = SinglePointCalculator(energy=energies[10], forces=forces[10], atoms=teststructure)\n",
    "\n",
    "### Lets verify that the data was input correctly\n",
    "print(teststructure.get_potential_energy(), \"=\",energies[10])\n",
    "print(\"\")\n",
    "print(teststructure.get_forces(), \"=\", forces[10])\n",
    "\n",
    "### Now we write out file to \"test.extxyz\". You can open this file to see that things have been properly written\n",
    "from ase.io import write\n",
    "write(\"test.extxyz\", teststructure, format='extxyz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77619b99-e6e9-488f-8a3a-b59f3b856767",
   "metadata": {},
   "source": [
    "### Lets now print the file \"test.extxyz\" to make sure the energy and forces were written properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e4d7f07-5084-4379-9767-7fa63648d05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "Lattice=\"4.11427362168317 -0.0 0.0 -0.0 4.52931324091741 0.0 0.0 0.0 5.08213668220882\" Properties=species:S:1:pos:R:3:forces:R:3 energy=-92.87576295593863 pbc=\"T T T\"\n",
      "O        0.93124927       4.18687216       3.29139399       3.33797564       1.62464736      -3.84386113\n",
      "O        1.13533575       1.90503942       4.54970986      -3.67823520      -0.14456652      -1.00140032\n",
      "O        3.13548544       2.44313150       3.00441673      -2.09448968       2.85214464      -0.57546884\n",
      "O        2.99264230       0.38097914       4.67438489       1.98640845      -2.08304851      -0.90779581\n",
      "O        2.96687506       0.34653430       2.08259701       0.53734864      -0.38469393      -1.69818315\n",
      "O        3.08859044       2.50767127       0.34202930       1.66344134       2.51305205       2.59690119\n",
      "O        1.05595407       1.98810120       1.86008787       2.00028782      -1.18841462       1.89392922\n",
      "O        0.84910803       4.12291608       0.67747775       1.04344342       1.11123829       1.88026772\n",
      "Si       0.01977235       1.17445166       0.87356706       0.18837934       0.51097662      -3.16485836\n",
      "Si       2.35796014       3.41799866       1.56293038      -2.90944130      -4.04642925       1.61256092\n",
      "Si       0.01798658       3.59585117       4.38186367      -2.95077166      -5.85190149       0.55686183\n",
      "Si       1.89052057       1.00477848       3.25843458       0.87546735       5.08684284       2.65092949\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"test.extxyz\",\"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a5c5a7-7b93-42ee-ad08-1d6f1a685b25",
   "metadata": {},
   "source": [
    "### Energy is written on line 2 and the forces are written next to the positions in columns 4, 5 and 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa88d815-955b-481e-bf01-19c52305759f",
   "metadata": {},
   "source": [
    "### Now that we know its working, lets go ahead and split up our structure, energies and forces into a training set and a testing set. The training set will be used to train the model, and the testing set will be used to verify the model works properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acdd7f1a-67b0-43bc-8027-332b5a8d09c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original index order: [  0   1   2 ... 997 998 999]\n",
      "Shuffled index order: [493  43 598 ... 786 877 575]\n",
      "\n",
      "Number of training points: 900\n",
      "Number of testing points: 100\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(threshold=100)\n",
    "\n",
    "### First lets make a list of the indicies from 0 to the number of structures we have\n",
    "indicies = np.arange(0, len(structures))\n",
    "print(\"Original index order:\", indicies)\n",
    "\n",
    "### Now lets shuffle these so that there is a random distribution of data in the training and testing sets\n",
    "np.random.shuffle(indicies)\n",
    "print(\"Shuffled index order:\", indicies)\n",
    "\n",
    "### Now lets define what percentage of the data we want in the training and testing sets. \n",
    "### Here, we choose that 90% of the data goes to the training set and 10% for the testing set\n",
    "### This needs to be an integer, so we wrap it with the int() function\n",
    "Num_train = int(0.9*len(structures))\n",
    "Num_test = len(structures) - Num_train\n",
    "print(\"\")\n",
    "print(\"Number of training points:\", Num_train)\n",
    "print(\"Number of testing points:\", Num_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27507f20-5fb4-4016-9b13-cb036bf3455c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of training structures: 900\n"
     ]
    }
   ],
   "source": [
    "### Now lets loop through all the structures and create a list of training structures\n",
    "training_structures = []\n",
    "for i in range(0, Num_train):\n",
    "    ### This pulls one random structure from the structures list by selecting the ith element of indicies, which we shuffled before\n",
    "    training_structure = structures[indicies[i]]\n",
    "    training_structure.calc = SinglePointCalculator(energy=energies[indicies[i]], forces=forces[indicies[i]], atoms=training_structure)\n",
    "    training_structures.append(training_structure)\n",
    "\n",
    "### Lets make sure the list is the right length\n",
    "print(\"\")\n",
    "print(\"Number of training structures:\", len(training_structures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a893a2ef-1c38-4932-8923-b794a8b28fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of testing structures: 100\n"
     ]
    }
   ],
   "source": [
    "### We can now do the same thing for the testing structures.\n",
    "testing_structures = []\n",
    "\n",
    "### We now have i go from Num_train (900) to Num_train+Num_test (900+100 = 1000) to put the rest of structures into the testing set\n",
    "for i in range(Num_train, Num_train+Num_test):\n",
    "    ### This pulls one random structure from the structures list by selecting the ith element of indicies, which we shuffled before\n",
    "    testing_structure = structures[indicies[i]]\n",
    "    testing_structure.calc = SinglePointCalculator(energy=energies[indicies[i]], forces=forces[indicies[i]], atoms=testing_structure)\n",
    "    testing_structures.append(testing_structure)\n",
    "\n",
    "### Lets make sure the list is the right length\n",
    "print(\"\")\n",
    "print(\"Number of testing structures:\", len(testing_structures))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5a0e8e-2265-42ae-ab91-c9225e25adb7",
   "metadata": {},
   "source": [
    "### Now that we have both the training and testing sets, lets write them to their respective extxyz files so that NEQUIP can read them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17bc685e-7648-4453-9770-58cc7a7d8b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase.io import write\n",
    "write(\"train.extxyz\", training_structures, format='extxyz')\n",
    "write(\"test.extxyz\", testing_structures, format='extxyz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f52f94-1851-4995-8ed8-6426ada6e712",
   "metadata": {},
   "source": [
    "### Finally, it is time to run the training of the ML Potential. There is a configuration file included in the files called train.yaml which contains the training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20234e82-9158-4b42-a8c9-d600f1f5172e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# a simple example config file\n",
      "\n",
      "# Two folders will be used during the training: 'root'/process and 'root'/'run_name'\n",
      "# run_name contains logfiles and saved models\n",
      "# process contains processed data sets\n",
      "# if 'root'/'run_name' exists, 'root'/'run_name'_'year'-'month'-'day'-'hour'-'min'-'s' will be used instead.\n",
      "root: results/SiO_training\n",
      "run_name: SiO-training-run\n",
      "seed: 123                                                                         # model seed\n",
      "dataset_seed: 456                                                                 # data set seed\n",
      "default_dtype: float32\n",
      "append: true                                                                      # set true if a restarted run should append to the previous log file\n",
      "device: cpu\n",
      "\n",
      "# see https://arxiv.org/abs/2304.10061 for discussion of numerical precision\n",
      "\n",
      "# network\n",
      "r_max: 4.0                                                                        # cutoff radius in length units, here Angstrom, this is an important hyperparamter to scan\n",
      "num_layers: 4                                                                     # number of interaction blocks, we find 3-5 to work best\n",
      "l_max: 2                                                                          # the maximum irrep order (rotation order) for the network's features, l=2 is accurate but slower, l=1 if you want to be faster but less accurte\n",
      "parity: true                                                                      # whether to include features with odd mirror parity; often turning parity off gives equally good results but faster networks, so do consider this\n",
      "num_features: 32                                                                  # the multiplicity of the features, 32 is a good default for accurate network, if you want to be more accurate, go larger, if you want to be faster, go lower\n",
      "nonlinearity_type: gate                                                           # may be 'gate' or 'norm', 'gate' is recommended\n",
      "\n",
      "# scalar nonlinearities to use — available options are silu, ssp (shifted softplus), tanh, and abs.\n",
      "# Different nonlinearities are specified for e (even) and o (odd) parity;\n",
      "# note that only tanh and abs are correct for o (odd parity)\n",
      "# silu typically works best for even \n",
      "nonlinearity_scalars:\n",
      "  e: silu\n",
      "  o: tanh\n",
      "\n",
      "nonlinearity_gates:\n",
      "  e: silu\n",
      "  o: tanh\n",
      "\n",
      "# radial network basis\n",
      "num_basis: 8                                                                      # number of basis functions used in the radial basis, 8 usually works best\n",
      "BesselBasis_trainable: true                                                       # set true to train the bessel weights\n",
      "PolynomialCutoff_p: 6                                                             # p-exponent used in polynomial cutoff function, smaller p corresponds to stronger decay with distance\n",
      "\n",
      "# radial network\n",
      "invariant_layers: 2                                                               # number of radial layers, usually 1-3 works best, smaller is faster\n",
      "invariant_neurons: 64                                                             # number of hidden neurons in radial function, smaller is faster\n",
      "avg_num_neighbors: auto                                                           # number of neighbors to divide by, null => no normalization, auto computes it based on dataset \n",
      "use_sc: true                                                                      # use self-connection or not, usually gives big improvement\n",
      "\n",
      "\n",
      "\n",
      "# for extxyz file\n",
      "dataset: ase\n",
      "dataset_file_name: train.extxyz\n",
      "ase_args:\n",
      "  format: extxyz\n",
      "# A list of atomic types to be found in the data. The NequIP types will be named with the chemical symbols, and inputs with the correct atomic numbers will be mapped to the corresponding types.\n",
      "chemical_symbols:\n",
      "  - O\n",
      "  - Si\n",
      "\n",
      "# logging\n",
      "wandb: false                                                                        # we recommend using wandb for logging\n",
      "\n",
      "verbose: info                                                                      # the same as python logging, e.g. warning, info, debug, error; case insensitive\n",
      "log_batch_freq: 10                                                                # batch frequency, how often to print training errors withinin the same epoch\n",
      "log_epoch_freq: 1                                                                  # epoch frequency, how often to print \n",
      "save_checkpoint_freq: -1                                                           # frequency to save the intermediate checkpoint. no saving of intermediate checkpoints when the value is not positive.\n",
      "save_ema_checkpoint_freq: -1                                                       # frequency to save the intermediate ema checkpoint. no saving of intermediate checkpoints when the value is not positive.\n",
      "\n",
      "# training\n",
      "n_train: 850                                                                       # number of training data\n",
      "n_val: 50                                                                          # number of validation data\n",
      "learning_rate: 0.005                                                               # learning rate, we found values between 0.01 and 0.005 to work best - this is often one of the most important hyperparameters to tune\n",
      "batch_size: 5                                                                      # batch size, we found it important to keep this small for most applications including forces (1-5); for energy-only training, higher batch sizes work better\n",
      "validation_batch_size: 10                                                          # batch size for evaluating the model during validation. This does not affect the training results, but using the highest value possible (<=n_val) without running out of memory will speed up your training.\n",
      "max_epochs: 30                                                                 # stop training after _ number of epochs, we set a very large number, as e.g. 1million and then just use early stopping and not train the full number of epochs\n",
      "train_val_split: random                                                            # can be random or sequential. if sequential, first n_train elements are training, next n_val are val, else random, usually random is the right choice\n",
      "shuffle: true                                                                      # if true, the data loader will shuffle the data, usually a good idea\n",
      "metrics_key: validation_loss                                                       # metrics used for scheduling and saving best model. Options: `set`_`quantity`, set can be either \"train\" or \"validation, \"quantity\" can be loss or anything that appears in the validation batch step header, such as f_mae, f_rmse, e_mae, e_rmse\n",
      "use_ema: true                                                                      # if true, use exponential moving average on weights for val/test, usually helps a lot with training, in particular for energy errors\n",
      "ema_decay: 0.99                                                                    # ema weight, typically set to 0.99 or 0.999\n",
      "ema_use_num_updates: true                                                          # whether to use number of updates when computing averages\n",
      "report_init_validation: true                                                       # if True, report the validation error for just initialized model\n",
      "\n",
      "# early stopping based on metrics values.\n",
      "early_stopping_patiences:                                                          # stop early if a metric value stopped decreasing for n epochs\n",
      "  validation_loss: 50\n",
      "\n",
      "early_stopping_lower_bounds:                                                       # stop early if a metric value is lower than the bound\n",
      "  LR: 1.0e-5\n",
      "\n",
      "early_stopping_upper_bounds:                                                       # stop early if the training appears to have exploded\n",
      "  validation_loss: 1.0e+4\n",
      "\n",
      "# loss function\n",
      "loss_coeffs:                                                                        \n",
      "  forces: 1                                                                        # if using PerAtomMSELoss, a default weight of 1:1 on each should work well\n",
      "  total_energy:                                                                    \n",
      "    - 1\n",
      "    - PerAtomMSELoss\n",
      "\n",
      "# output metrics\n",
      "metrics_components:\n",
      "  - - forces                               # key \n",
      "    - mae                                  # \"rmse\" or \"mae\"\n",
      "  - - forces\n",
      "    - rmse\n",
      "  - - forces\n",
      "    - mae\n",
      "    - PerSpecies: True                     # if true, per species contribution is counted separately\n",
      "      report_per_component: False          # if true, statistics on each component (i.e. fx, fy, fz) will be counted separately\n",
      "  - - forces                                \n",
      "    - rmse                                  \n",
      "    - PerSpecies: True                     \n",
      "      report_per_component: False    \n",
      "  - - total_energy\n",
      "    - mae    \n",
      "  - - total_energy\n",
      "    - mae\n",
      "    - PerAtom: True                        # if true, energy is normalized by the number of atoms\n",
      "\n",
      "# optimizer, may be any optimizer defined in torch.optim\n",
      "# the name `optimizer_name`is case sensitive\n",
      "# IMPORTANT: for NequIP (not for Allegro), we find that in most cases AMSGrad strongly improves\n",
      "# out-of-distribution generalization over Adam. We highly recommed trying both AMSGrad (by setting\n",
      "# optimizer_amsgrad: true) and Adam (by setting optimizer_amsgrad: false)\n",
      "optimizer_name: Adam                                                             \n",
      "optimizer_amsgrad: true\n",
      "\n",
      "# lr scheduler, currently only supports the two options listed in full.yaml, i.e. on-pleteau and cosine annealing with warm restarts, if you need more please file an issue\n",
      "# here: on-plateau, reduce lr by factory of lr_scheduler_factor if metrics_key hasn't improved for lr_scheduler_patience epoch\n",
      "lr_scheduler_name: ReduceLROnPlateau\n",
      "lr_scheduler_patience: 100\n",
      "lr_scheduler_factor: 0.5\n",
      "\n",
      "# we provide a series of options to shift and scale the data\n",
      "# these are for advanced use and usually the defaults work very well\n",
      "# the default is to scale the atomic energy and forces by scaling them by the force standard deviation and to shift the energy by the mean atomic energy\n",
      "# in certain cases, it can be useful to have a trainable shift/scale and to also have species-dependent shifts/scales for each atom\n",
      "\n",
      "# initial atomic energy shift for each species. default to the mean of per atom energy. Optional\n",
      "# the value can be a constant float value, an array for each species, or a string that defines a statistics over the training dataset\n",
      "# if numbers are explicitly provided, they must be in the same energy units as the training data\n",
      "per_species_rescale_shifts: dataset_per_atom_total_energy_mean\n",
      "\n",
      "# initial atomic energy scale for each species. Optional.\n",
      "# the value can be a constant float value, an array for each species, or a string\n",
      "# if numbers are explicitly provided, they must be in the same energy units as the training data\n",
      "per_species_rescale_scales: dataset_forces_rms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Here we print train.yaml out for convenience\n",
    "with open(\"train.yaml\",\"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cbcc91-70fa-41c0-a82f-94bc06a214d1",
   "metadata": {},
   "source": [
    "### The important bits in this file are the dataset, dataset_file_name, chemical_symbols, n_train, n_val and max_epochs. <br>\n",
    "#### -- For dataset, dataset_file_name, chemical_symbols, those will have to be changed if you want to do training on another dataset or different types of atoms. Here, we are training structures with Si and O, so we put those as the chemical symbols \n",
    "#### -- For n_train and n_val, this sets the split of training and validation. What this means is that out of our 900 training structures, we will actually train on 850 of them and then use the remaining 50 to make sure the model isn't overfitting. Overfitting means that the model has ONLY learned the training data and nothing else. If the model is overfitting, the error for the training set will go rapidly down, but the error for the validation set will start to increase. This is very bad and hence we must reserve some data for validation \n",
    "#### -- Now for max_epochs, this just sets how many loops over the training data we do. Every time we loop over the training data, the model learns a bit more and becomes a bit better. Here we set it to 5 for a quick demonstration but in reality you would want 50-100."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3dae7e-a2ff-4b81-802d-721c1fd2a273",
   "metadata": {},
   "source": [
    "### Now lets run the training loop. This will take a few minutes so just sit back and wait. We print out \"I am done\" after it finishes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437772f2-e6bf-430c-bef5-7ea2adc4a493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.3.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/nequip/scripts/train.py:180: UserWarning: default_dtype=float32 but we strongly recommend float64\n",
      "  warnings.warn(\n",
      "Torch device: cpu\n",
      "Processing dataset...\n",
      "Loaded data: Batch(atomic_numbers=[10800, 1], batch=[10800], cell=[900, 3, 3], edge_cell_shift=[335110, 3], edge_index=[2, 335110], forces=[10800, 3], pbc=[900, 3], pos=[10800, 3], ptr=[901], total_energy=[900, 1])\n",
      "    processed data size: ~9.40 MB\n",
      "Cached processed data to disk\n",
      "Done!\n",
      "Successfully loaded the data set of type ASEDataset(900)...\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "Replace string dataset_forces_rms to 3.422625780105591\n",
      "Replace string dataset_per_atom_total_energy_mean to -7.469429969787598\n",
      "Atomic outputs are scaled by: [O, Si: 3.422626], shifted by [O, Si: -7.469430].\n",
      "Replace string dataset_forces_rms to 3.422625780105591\n",
      "Initially outputs are globally scaled by: 3.422625780105591, total_energy are globally shifted by None.\n",
      "Successfully built the network...\n",
      "Number of weights: 363096\n",
      "Number of trainable weights: 363096\n",
      "! Starting training ...\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "      0     5         1.18         1.16       0.0242         2.72         3.68         2.63         2.89         2.76         3.53         3.96         3.75         5.81        0.485\n",
      "\n",
      "\n",
      "  Initialization     #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Initial Validation          0    2.948    0.005        0.925       0.0164        0.941         2.55         3.29          2.5         2.65         2.57         3.19         3.48         3.34         4.81        0.401\n",
      "Wall time: 2.9479116670554504\n",
      "! Best model        0    0.941\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "      1    10        0.376        0.328       0.0474         1.46         1.96         1.35         1.68         1.51          1.8         2.25         2.02         8.86        0.738\n",
      "      1    20        0.346        0.322        0.024         1.32         1.94         1.04         1.87         1.45         1.61         2.48         2.04         6.28        0.523\n",
      "      1    30        0.207        0.201      0.00552         1.19         1.53        0.959         1.66         1.31         1.29         1.93         1.61         2.95        0.246\n",
      "      1    40       0.0926       0.0918     0.000789        0.799         1.04        0.647          1.1        0.875        0.833         1.36         1.09        0.964       0.0804\n",
      "      1    50       0.0811       0.0804     0.000691        0.741        0.971        0.584         1.06        0.819         0.73         1.33         1.03         1.01       0.0838\n",
      "      1    60        0.107        0.106     0.000573        0.761         1.12        0.542          1.2        0.871        0.675         1.68         1.18        0.779       0.0649\n",
      "      1    70       0.0555       0.0554     0.000134        0.617        0.805        0.494        0.862        0.678        0.626         1.08        0.852         0.43       0.0358\n",
      "      1    80       0.0614       0.0613     7.28e-05        0.647        0.847        0.517        0.907        0.712        0.666         1.13        0.896        0.288        0.024\n",
      "      1    90       0.0457       0.0455     0.000222        0.537         0.73         0.38        0.849        0.615        0.474         1.07        0.773        0.504        0.042\n",
      "      1   100       0.0279       0.0277     0.000178        0.426         0.57         0.33        0.617        0.473        0.422        0.786        0.604         0.51       0.0425\n",
      "      1   110       0.0303       0.0301     0.000175        0.466        0.594        0.409        0.579        0.494        0.526         0.71        0.618         0.49       0.0408\n",
      "      1   120       0.0195       0.0189      0.00056        0.371        0.471        0.313        0.488          0.4        0.384        0.609        0.496        0.952       0.0793\n",
      "      1   130       0.0233       0.0217       0.0016        0.389        0.504        0.322        0.522        0.422        0.421        0.637        0.529          1.6        0.134\n",
      "      1   140       0.0183       0.0181     0.000254        0.356         0.46        0.303        0.463        0.383        0.404        0.556         0.48        0.625       0.0521\n",
      "      1   150       0.0147       0.0145     0.000191        0.324        0.412         0.27        0.431        0.351        0.342        0.524        0.433        0.533       0.0444\n",
      "      1   160       0.0131        0.013     7.72e-05        0.292        0.391        0.242        0.393        0.317        0.312        0.513        0.412        0.319       0.0266\n",
      "      1   170       0.0223       0.0223     2.74e-05        0.404        0.511        0.347        0.519        0.433        0.438        0.631        0.535        0.151       0.0126\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "      1     5       0.0215       0.0215     3.89e-05        0.384        0.501        0.324        0.504        0.414        0.425        0.627        0.526        0.198       0.0165\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train               1  127.353    0.005        0.115      0.00354        0.119        0.724         1.16        0.614        0.943        0.779         1.01         1.42         1.21         1.51        0.126\n",
      "! Validation          1  127.353    0.005       0.0155     2.54e-05       0.0156         0.32        0.427        0.275         0.41        0.343        0.369        0.523        0.446        0.154       0.0128\n",
      "Wall time: 127.35316733305808\n",
      "! Best model        1    0.016\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "      2    10       0.0142       0.0141     7.81e-05        0.306        0.406        0.256        0.406        0.331        0.337        0.518        0.428        0.346       0.0288\n",
      "      2    20       0.0168       0.0167     7.61e-05        0.341        0.442         0.31        0.403        0.356         0.39        0.531        0.461        0.261       0.0217\n",
      "      2    30       0.0105       0.0105     7.38e-06        0.252        0.351        0.204         0.35        0.277        0.265        0.477        0.371       0.0954      0.00795\n",
      "      2    40       0.0243       0.0242     8.26e-05        0.351        0.533        0.329        0.397        0.363        0.557         0.48        0.518        0.342       0.0285\n",
      "      2    50       0.0115       0.0112      0.00025        0.264        0.363        0.229        0.335        0.282        0.286        0.481        0.383        0.645       0.0537\n",
      "      2    60        0.012       0.0118     0.000181        0.275        0.373        0.219        0.388        0.303         0.28        0.509        0.395        0.519       0.0432\n",
      "      2    70      0.00898      0.00833      0.00065        0.238        0.312         0.22        0.274        0.247        0.288        0.356        0.322         1.04       0.0864\n",
      "      2    80       0.0122       0.0121     2.28e-05        0.288        0.377         0.25        0.365        0.307        0.332        0.454        0.393         0.17       0.0142\n",
      "      2    90      0.00766       0.0073     0.000366        0.227        0.292         0.19        0.299        0.245        0.243        0.372        0.307        0.785       0.0654\n",
      "      2   100      0.00632      0.00631     1.59e-05        0.192        0.272        0.168        0.241        0.205        0.254        0.305        0.279         0.14       0.0117\n",
      "      2   110      0.00748      0.00735     0.000133        0.221        0.293        0.208        0.247        0.227        0.275        0.327        0.301        0.453       0.0378\n",
      "      2   120      0.00774      0.00769     5.38e-05        0.229          0.3          0.2        0.286        0.243        0.264        0.362        0.313        0.299       0.0249\n",
      "      2   130      0.00746      0.00745      6.2e-06        0.197        0.295        0.156         0.28        0.218        0.201        0.425        0.313       0.0918      0.00765\n",
      "      2   140      0.00301        0.003     4.78e-06        0.145        0.188        0.126        0.182        0.154        0.165        0.226        0.195       0.0789      0.00657\n",
      "      2   150      0.00342      0.00339     2.35e-05        0.153        0.199         0.14         0.18         0.16        0.181        0.232        0.206        0.192        0.016\n",
      "      2   160      0.00685      0.00684     6.31e-06        0.197        0.283        0.195          0.2        0.197        0.272        0.305        0.288       0.0981      0.00817\n",
      "      2   170      0.00348       0.0034     8.38e-05        0.158          0.2        0.149        0.178        0.163        0.192        0.214        0.203        0.373       0.0311\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "      2     5      0.00587      0.00586     1.11e-05        0.191        0.262        0.168        0.235        0.202        0.236        0.308        0.272        0.106       0.0088\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train               2  249.881    0.005        0.009     0.000176      0.00918        0.237        0.325        0.207        0.297        0.252         0.28          0.4         0.34        0.417       0.0348\n",
      "! Validation          2  249.881    0.005      0.00461     5.98e-06      0.00461        0.165        0.232        0.146        0.203        0.175        0.208        0.275        0.241       0.0744       0.0062\n",
      "Wall time: 249.88202570797876\n",
      "! Best model        2    0.005\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "      3    10      0.00639      0.00633      5.7e-05          0.2        0.272         0.19         0.22        0.205        0.278        0.261         0.27        0.281       0.0234\n",
      "      3    20       0.0039      0.00389     6.44e-06        0.164        0.214        0.159        0.173        0.166         0.21        0.221        0.215       0.0848      0.00707\n",
      "      3    30      0.00304      0.00302     2.58e-05        0.148        0.188        0.139        0.164        0.152        0.175        0.212        0.193        0.201       0.0167\n",
      "      3    40      0.00271      0.00266     4.43e-05        0.129        0.177         0.11        0.168        0.139         0.14        0.233        0.187         0.27       0.0225\n",
      "      3    50      0.00293      0.00293     6.64e-06        0.142        0.185        0.126        0.174         0.15        0.162        0.224        0.193       0.0987      0.00822\n",
      "      3    60      0.00746      0.00744     1.96e-05        0.213        0.295          0.2         0.24         0.22        0.276         0.33        0.303        0.162       0.0135\n",
      "      3    70      0.00506      0.00504     1.93e-05        0.189        0.243        0.171        0.226        0.198        0.213        0.294        0.254        0.165       0.0137\n",
      "      3    80      0.00734       0.0073     4.12e-05         0.23        0.292        0.227        0.235        0.231        0.295        0.287        0.291        0.243       0.0203\n",
      "      3    90      0.00399      0.00323     0.000763        0.156        0.195        0.155        0.156        0.156        0.193        0.197        0.195         1.13       0.0943\n",
      "      3   100      0.00262      0.00254     8.33e-05        0.134        0.172         0.12        0.164        0.142        0.153        0.206        0.179        0.371        0.031\n",
      "      3   110      0.00144      0.00137     6.87e-05        0.105        0.127       0.0949        0.127        0.111        0.112        0.153        0.132        0.333       0.0277\n",
      "      3   120      0.00617      0.00611     6.28e-05        0.193        0.267        0.155        0.268        0.212        0.202        0.365        0.283        0.257       0.0214\n",
      "      3   130      0.00455      0.00452     3.39e-05        0.164         0.23        0.144        0.204        0.174        0.199        0.282        0.241        0.223       0.0186\n",
      "      3   140      0.00334       0.0033      4.6e-05        0.152        0.197        0.125        0.206        0.166        0.158        0.257        0.207        0.263        0.022\n",
      "      3   150      0.00515      0.00508     6.86e-05        0.187        0.244        0.159        0.243        0.201        0.207        0.304        0.256        0.335       0.0279\n",
      "      3   160      0.00415      0.00403     0.000121        0.161        0.217        0.137        0.209        0.173        0.181        0.276        0.228         0.44       0.0367\n",
      "      3   170      0.00356      0.00355     5.52e-06        0.158        0.204        0.139        0.196        0.168        0.189        0.232         0.21       0.0893      0.00745\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "      3     5       0.0036      0.00359     4.36e-06        0.146        0.205        0.127        0.184        0.156        0.178         0.25        0.214       0.0666      0.00555\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train               3  383.638    0.005      0.00444     0.000101      0.00454        0.167        0.228        0.152        0.197        0.174        0.208        0.264        0.236        0.332       0.0277\n",
      "! Validation          3  383.638    0.005       0.0027     2.76e-06       0.0027        0.127        0.178        0.112        0.157        0.134        0.158        0.212        0.185       0.0521      0.00434\n",
      "Wall time: 383.63930587505456\n",
      "! Best model        3    0.003\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "      4    10       0.0048      0.00479     8.96e-06        0.175        0.237        0.145        0.235         0.19        0.189        0.311         0.25        0.104      0.00869\n",
      "      4    20      0.00148      0.00144     3.48e-05        0.105         0.13       0.0972         0.12        0.109        0.124        0.141        0.133        0.239       0.0199\n",
      "      4    30      0.00248      0.00244     4.39e-05        0.129        0.169        0.121        0.145        0.133        0.161        0.183        0.172        0.266       0.0222\n",
      "      4    40      0.00159      0.00155     4.26e-05        0.107        0.135       0.0981        0.125        0.111        0.121        0.158         0.14        0.267       0.0223\n",
      "      4    50      0.00273      0.00272     1.08e-05        0.133        0.178        0.116        0.168        0.142        0.159        0.212        0.185        0.117      0.00974\n",
      "      4    60      0.00441       0.0044     1.45e-05        0.159        0.227         0.16        0.159        0.159        0.235        0.209        0.222         0.14       0.0117\n",
      "      4    70      0.00228      0.00222        6e-05        0.128        0.161         0.11        0.164        0.137        0.139        0.199        0.169        0.309       0.0258\n",
      "      4    80       0.0027      0.00268      2.7e-05        0.136        0.177        0.132        0.144        0.138        0.175        0.181        0.178        0.195       0.0162\n",
      "      4    90      0.00263      0.00244     0.000193        0.128        0.169        0.124        0.136         0.13        0.171        0.166        0.168        0.567       0.0472\n",
      "      4   100      0.00203      0.00203     1.88e-07        0.119        0.154        0.108        0.142        0.125        0.138        0.182         0.16       0.0152      0.00127\n",
      "      4   110      0.00246      0.00245     5.57e-06        0.127         0.17         0.12        0.141        0.131        0.166        0.177        0.171       0.0862      0.00719\n",
      "      4   120      0.00298      0.00296     2.19e-05        0.125        0.186        0.118        0.139        0.129        0.182        0.194        0.188         0.18        0.015\n",
      "      4   130     0.000855      0.00085     4.97e-06       0.0826       0.0998       0.0778       0.0921        0.085       0.0941         0.11        0.102       0.0828       0.0069\n",
      "      4   140      0.00246      0.00246     1.96e-06        0.128         0.17        0.119        0.147        0.133        0.162        0.183        0.173       0.0496      0.00414\n",
      "      4   150      0.00219      0.00202     0.000177        0.117        0.154        0.107        0.136        0.122        0.145         0.17        0.157        0.543       0.0453\n",
      "      4   160      0.00207       0.0019     0.000176        0.114        0.149        0.098        0.146        0.122        0.125        0.188        0.157        0.544       0.0453\n",
      "      4   170      0.00553      0.00533     0.000206         0.19         0.25        0.184        0.203        0.194        0.243        0.263        0.253        0.574       0.0478\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "      4     5      0.00259      0.00259     3.83e-06        0.124        0.174         0.11        0.152        0.131        0.153         0.21        0.182        0.059      0.00492\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train               4  514.991    0.005      0.00267     5.49e-05      0.00272         0.13        0.177         0.12         0.15        0.135        0.164          0.2        0.182        0.243       0.0203\n",
      "! Validation          4  514.991    0.005       0.0019     2.33e-06       0.0019        0.108        0.149       0.0961        0.131        0.114        0.133        0.177        0.155       0.0494      0.00411\n",
      "Wall time: 514.9933668330777\n",
      "! Best model        4    0.002\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "      5    10      0.00207      0.00189     0.000179        0.114        0.149        0.102        0.137         0.12         0.13         0.18        0.155        0.545       0.0454\n",
      "      5    20      0.00488      0.00462     0.000259        0.166        0.233         0.14        0.219        0.179        0.186        0.305        0.246        0.649        0.054\n",
      "      5    30      0.00361      0.00347     0.000139        0.126        0.202        0.117        0.146        0.131        0.209        0.186        0.198        0.473       0.0394\n",
      "      5    40      0.00184      0.00181     3.18e-05        0.112        0.146        0.107         0.12        0.114        0.141        0.155        0.148        0.212       0.0177\n",
      "      5    50      0.00218      0.00216     1.61e-05        0.123        0.159        0.113        0.144        0.129        0.146        0.183        0.165        0.132        0.011\n",
      "      5    60      0.00139      0.00137     2.42e-05       0.0999        0.127       0.0931        0.113        0.103        0.116        0.145        0.131        0.196       0.0163\n",
      "      5    70      0.00307      0.00292     0.000155        0.123        0.185        0.116        0.139        0.127        0.178        0.198        0.188        0.506       0.0421\n",
      "      5    80      0.00235      0.00204     0.000313        0.112        0.155        0.105        0.126        0.116        0.144        0.175        0.159        0.722       0.0601\n",
      "      5    90      0.00291      0.00285     6.34e-05        0.121        0.183        0.117        0.128        0.123        0.192        0.162        0.177        0.324        0.027\n",
      "      5   100       0.0017       0.0017     9.66e-07        0.111        0.141        0.108        0.116        0.112        0.136        0.151        0.144        0.035      0.00292\n",
      "      5   110      0.00304      0.00303     1.04e-05        0.148        0.188        0.147         0.15        0.148         0.19        0.185        0.187        0.114      0.00953\n",
      "      5   120      0.00196      0.00188     7.92e-05        0.118        0.148        0.109        0.134        0.122        0.145        0.154         0.15        0.357       0.0298\n",
      "      5   130       0.0016      0.00156     3.99e-05        0.101        0.135       0.0846        0.134        0.109         0.11        0.175        0.143        0.257       0.0214\n",
      "      5   140      0.00123      0.00112     0.000109       0.0903        0.114       0.0796        0.112       0.0956          0.1        0.139        0.119        0.428       0.0356\n",
      "      5   150       0.0011       0.0011      1.5e-06       0.0807        0.113       0.0729       0.0964       0.0847       0.0991        0.138        0.118       0.0367      0.00306\n",
      "      5   160      0.00138      0.00138     1.38e-06       0.0997        0.127       0.0895         0.12        0.105        0.113         0.15        0.132       0.0391      0.00326\n",
      "      5   170      0.00131       0.0013     3.17e-06        0.095        0.124       0.0866        0.112       0.0992        0.116        0.138        0.127       0.0624       0.0052\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "      5     5      0.00197      0.00197     2.17e-06        0.108        0.152       0.0979        0.129        0.113        0.135         0.18        0.158       0.0456       0.0038\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train               5  641.625    0.005      0.00216     8.06e-05      0.00224        0.118        0.159         0.11        0.134        0.122        0.148        0.178        0.163        0.298       0.0249\n",
      "! Validation          5  641.625    0.005      0.00146     1.57e-06      0.00146        0.095        0.131       0.0855        0.114       0.0998        0.118        0.153        0.136       0.0405      0.00337\n",
      "Wall time: 641.6257150419988\n",
      "! Best model        5    0.001\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "      6    10      0.00133      0.00126     6.45e-05       0.0982        0.122       0.0988       0.0969       0.0979        0.124        0.117         0.12        0.328       0.0274\n",
      "      6    20      0.00111      0.00111     7.19e-06       0.0905        0.114        0.087       0.0975       0.0923        0.109        0.122        0.116        0.101       0.0084\n",
      "      6    30      0.00177      0.00171     6.23e-05        0.108        0.141        0.111        0.103        0.107        0.145        0.134         0.14        0.322       0.0268\n",
      "      6    40      0.00151      0.00149     1.71e-05        0.105        0.132        0.103         0.11        0.106        0.129        0.137        0.133        0.164       0.0137\n",
      "      6    50      0.00167      0.00162     5.01e-05        0.106        0.138       0.0988        0.119        0.109        0.127        0.157        0.142        0.285       0.0238\n",
      "      6    60      0.00201        0.002     2.91e-06        0.118        0.153        0.114        0.125         0.12        0.151        0.158        0.154       0.0601      0.00501\n",
      "      6    70      0.00143      0.00141     1.08e-05       0.0989        0.129       0.0963        0.104          0.1        0.124        0.138        0.131         0.13       0.0109\n",
      "      6    80      0.00113      0.00113     1.81e-06       0.0897        0.115       0.0792        0.111        0.095        0.102        0.137         0.12       0.0467      0.00389\n",
      "      6    90      0.00135      0.00131     3.71e-05       0.0948        0.124       0.0796        0.125        0.102       0.0999        0.162        0.131        0.247       0.0206\n",
      "      6   100      0.00111      0.00107     3.04e-05       0.0861        0.112       0.0837       0.0909       0.0873        0.106        0.123        0.115        0.223       0.0186\n",
      "      6   110     0.000898     0.000846     5.21e-05       0.0765       0.0995        0.075       0.0795       0.0772       0.0977        0.103          0.1        0.293       0.0244\n",
      "      6   120      0.00138      0.00118     0.000197       0.0917        0.117       0.0901       0.0948       0.0924        0.114        0.125        0.119        0.575       0.0479\n",
      "      6   130      0.00189       0.0018     8.24e-05        0.115        0.145        0.102        0.142        0.122        0.126        0.178        0.152         0.37       0.0308\n",
      "      6   140     0.000999     0.000932     6.69e-05       0.0822        0.104       0.0819       0.0828       0.0823        0.104        0.106        0.105        0.334       0.0278\n",
      "      6   150      0.00115      0.00114     3.22e-06       0.0882        0.116       0.0792        0.106       0.0927        0.101         0.14        0.121       0.0662      0.00552\n",
      "      6   160      0.00226      0.00221     5.24e-05        0.126        0.161        0.109         0.16        0.135        0.138        0.199        0.168        0.296       0.0246\n",
      "      6   170      0.00181      0.00181     5.21e-06        0.112        0.145        0.104        0.128        0.116        0.134        0.166         0.15       0.0927      0.00772\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "      6     5      0.00161      0.00161     1.64e-06       0.0989        0.137       0.0899        0.117        0.103        0.124        0.161        0.142       0.0406      0.00338\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train               6  784.992    0.005      0.00157     3.87e-05      0.00161        0.101        0.135       0.0955        0.113        0.104        0.128        0.149        0.139        0.209       0.0175\n",
      "! Validation          6  784.992    0.005      0.00119     1.27e-06      0.00119       0.0869        0.118       0.0791        0.102       0.0907        0.108        0.136        0.122       0.0364      0.00303\n",
      "Wall time: 784.9926883330336\n",
      "! Best model        6    0.001\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "      7    10      0.00236      0.00233     2.58e-05        0.127        0.165        0.119        0.142        0.131        0.153        0.187         0.17        0.205       0.0171\n",
      "      7    20      0.00153      0.00152     3.28e-06       0.0992        0.134       0.0973        0.103          0.1        0.122        0.155        0.138       0.0729      0.00607\n",
      "      7    30      0.00102      0.00101      1.3e-05       0.0843        0.109       0.0762        0.101       0.0884       0.0971        0.129        0.113        0.142       0.0118\n",
      "      7    40      0.00179      0.00179     2.57e-06       0.0954        0.145        0.091        0.104       0.0975         0.15        0.133        0.142       0.0566      0.00471\n",
      "      7    50       0.0017      0.00169      9.4e-06       0.0992        0.141       0.0944        0.109        0.102        0.136         0.15        0.143        0.116      0.00966\n",
      "      7    60       0.0013      0.00127     2.42e-05       0.0897        0.122       0.0881       0.0929       0.0905        0.122        0.122        0.122        0.196       0.0164\n",
      "      7    70      0.00117      0.00116     1.02e-05       0.0933        0.117        0.084        0.112       0.0979        0.105        0.138        0.121        0.113      0.00944\n",
      "      7    80     0.000981     0.000967     1.46e-05       0.0819        0.106       0.0741       0.0974       0.0858       0.0937        0.128        0.111        0.149       0.0124\n",
      "      7    90      0.00128      0.00128      2.2e-06       0.0954        0.123        0.091        0.104       0.0976        0.116        0.134        0.125       0.0492       0.0041\n",
      "      7   100       0.0013      0.00125     5.43e-05       0.0945        0.121       0.0863        0.111       0.0986        0.109        0.141        0.125        0.296       0.0247\n",
      "      7   110      0.00155      0.00154     1.97e-06        0.104        0.134        0.099        0.115        0.107        0.131        0.141        0.136       0.0532      0.00444\n",
      "      7   120      0.00196      0.00193     2.26e-05        0.107        0.151       0.0995        0.121         0.11        0.141        0.168        0.155         0.18        0.015\n",
      "      7   130     0.000727     0.000724      3.1e-06       0.0727       0.0921       0.0751       0.0679       0.0715       0.0947       0.0866       0.0907        0.056      0.00466\n",
      "      7   140     0.000641      0.00064     3.66e-07        0.067       0.0866       0.0565       0.0879       0.0722       0.0736        0.108       0.0908       0.0203      0.00169\n",
      "      7   150      0.00112      0.00112     1.72e-06       0.0865        0.115       0.0818       0.0959       0.0889        0.109        0.124        0.117        0.046      0.00383\n",
      "      7   160     0.000996     0.000987     9.18e-06       0.0854        0.108       0.0857       0.0848       0.0852         0.11        0.102        0.106        0.121       0.0101\n",
      "      7   170      0.00141      0.00131     0.000106       0.0957        0.124       0.0958       0.0957       0.0957        0.126         0.12        0.123        0.417       0.0348\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "      7     5      0.00131      0.00131     1.31e-06       0.0907        0.124       0.0827        0.107       0.0947        0.113        0.144        0.128       0.0366      0.00305\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train               7  921.553    0.005      0.00127     2.46e-05       0.0013       0.0915        0.122       0.0868        0.101       0.0938        0.116        0.133        0.125        0.164       0.0136\n",
      "! Validation          7  921.553    0.005     0.000967     9.82e-07     0.000968       0.0791        0.106       0.0724       0.0925       0.0825       0.0978        0.122         0.11       0.0315      0.00262\n",
      "Wall time: 921.554281083052\n",
      "! Best model        7    0.001\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "      8    10      0.00257      0.00205      0.00052        0.118        0.155        0.105        0.144        0.125        0.137        0.186        0.162        0.931       0.0776\n",
      "      8    20      0.00163      0.00146     0.000162       0.0977        0.131       0.0938        0.105       0.0996        0.123        0.146        0.134        0.518       0.0432\n",
      "      8    30       0.0013      0.00124     5.29e-05       0.0864        0.121       0.0736        0.112       0.0928          0.1        0.154        0.127        0.295       0.0246\n",
      "      8    40      0.00348      0.00347     1.12e-05         0.13        0.202        0.137        0.116        0.127        0.193        0.219        0.206        0.132        0.011\n",
      "      8    50      0.00302        0.003     1.47e-05        0.145        0.188        0.134        0.168        0.151         0.18        0.203        0.191        0.146       0.0122\n",
      "      8    60      0.00225      0.00218     7.88e-05        0.127         0.16        0.123        0.137         0.13        0.158        0.162         0.16        0.358       0.0298\n",
      "      8    70      0.00172      0.00165     6.55e-05        0.114        0.139        0.111         0.12        0.115        0.135        0.146        0.141        0.328       0.0273\n",
      "      8    80       0.0016      0.00158      1.8e-05        0.105        0.136        0.104        0.108        0.106        0.135        0.138        0.137        0.152       0.0127\n",
      "      8    90     0.000762     0.000732     3.01e-05       0.0704       0.0926        0.064       0.0833       0.0736       0.0856        0.105       0.0954        0.224       0.0187\n",
      "      8   100      0.00115      0.00113     1.62e-05       0.0916        0.115        0.086        0.103       0.0943        0.108        0.129        0.118        0.155       0.0129\n",
      "      8   110      0.00106      0.00104     2.44e-05       0.0856         0.11       0.0843       0.0884       0.0863        0.107        0.118        0.112        0.199       0.0166\n",
      "      8   120     0.000945     0.000794     0.000151       0.0732       0.0964       0.0674       0.0849       0.0761       0.0908        0.107       0.0988        0.504        0.042\n",
      "      8   130      0.00118     0.000991     0.000187       0.0877        0.108       0.0856        0.092       0.0888        0.106        0.111        0.108        0.561       0.0467\n",
      "      8   140     0.000988     0.000957      3.1e-05       0.0837        0.106       0.0843       0.0827       0.0835        0.105        0.108        0.106        0.228        0.019\n",
      "      8   150     0.000894     0.000845     4.98e-05       0.0781       0.0995       0.0745       0.0853       0.0799       0.0917        0.113        0.103        0.287       0.0239\n",
      "      8   160      0.00106      0.00103     2.44e-05       0.0864         0.11       0.0903       0.0785       0.0844        0.115       0.0988        0.107        0.198       0.0165\n",
      "      8   170       0.0012      0.00109     0.000114       0.0871        0.113       0.0785        0.104       0.0914        0.102        0.132        0.117        0.437       0.0364\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "      8     5      0.00112      0.00111      1.7e-06       0.0829        0.114       0.0765       0.0957       0.0861        0.104        0.132        0.118       0.0373      0.00311\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train               8 1062.462    0.005      0.00135     9.54e-05      0.00145       0.0939        0.126        0.091       0.0995       0.0953        0.122        0.133        0.128         0.33       0.0275\n",
      "! Validation          8 1062.462    0.005     0.000831     1.12e-06     0.000832       0.0731       0.0986       0.0678       0.0839       0.0758       0.0912        0.112        0.102       0.0334      0.00278\n",
      "Wall time: 1062.4633580420632\n",
      "! Best model        8    0.001\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "      9    10      0.00105      0.00103     1.26e-05       0.0811         0.11       0.0764       0.0905       0.0835        0.102        0.126        0.114        0.135       0.0113\n",
      "      9    20      0.00113     0.000986     0.000141       0.0852        0.107       0.0848       0.0862       0.0855        0.105        0.112        0.108        0.484       0.0403\n",
      "      9    30     0.000673     0.000609     6.42e-05       0.0685       0.0844       0.0706       0.0643       0.0674       0.0869       0.0792       0.0831        0.327       0.0273\n",
      "      9    40      0.00128      0.00122     5.56e-05       0.0949         0.12       0.0926       0.0996       0.0961        0.115        0.128        0.122        0.298       0.0248\n",
      "      9    50      0.00134      0.00134     1.59e-06       0.0966        0.125       0.0867        0.116        0.102        0.113        0.147         0.13       0.0394      0.00328\n",
      "      9    60     0.000825     0.000779     4.51e-05       0.0741       0.0956       0.0705       0.0813       0.0759       0.0936       0.0994       0.0965        0.271       0.0226\n",
      "      9    70     0.000684     0.000665     1.88e-05       0.0672       0.0883        0.069       0.0635       0.0662       0.0914       0.0816       0.0865        0.177       0.0147\n",
      "      9    80     0.000887     0.000845      4.2e-05       0.0773       0.0995       0.0811       0.0696       0.0754        0.102       0.0945       0.0982        0.266       0.0222\n",
      "      9    90     0.000889     0.000863     2.57e-05       0.0768        0.101       0.0763       0.0777        0.077        0.105        0.091        0.098        0.204        0.017\n",
      "      9   100     0.000861      0.00083     3.03e-05       0.0725       0.0986       0.0668       0.0839       0.0753       0.0905        0.113        0.102        0.225       0.0187\n",
      "      9   110      0.00074      0.00073     1.04e-05       0.0709       0.0925       0.0684       0.0758       0.0721       0.0889       0.0992       0.0941        0.123       0.0103\n",
      "      9   120     0.000647     0.000638     9.82e-06       0.0688       0.0864       0.0624       0.0816        0.072        0.078        0.101       0.0896        0.125       0.0104\n",
      "      9   130        0.001     0.000949     5.25e-05       0.0851        0.105       0.0795       0.0964       0.0879        0.097        0.121        0.109        0.295       0.0246\n",
      "      9   140     0.000691     0.000685     5.71e-06       0.0727       0.0896       0.0771       0.0638       0.0705       0.0937       0.0807       0.0872       0.0969      0.00808\n",
      "      9   150      0.00094     0.000938     1.46e-06       0.0801        0.105       0.0804       0.0794       0.0799        0.105        0.104        0.105       0.0448      0.00373\n",
      "      9   160     0.000711     0.000711     2.53e-07       0.0724       0.0912       0.0704       0.0763       0.0734       0.0867       0.0997       0.0932       0.0174      0.00145\n",
      "      9   170     0.000887     0.000886     9.97e-07       0.0789        0.102       0.0745       0.0878       0.0811       0.0959        0.113        0.104       0.0364      0.00303\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "      9     5     0.000944     0.000942     1.26e-06       0.0771        0.105       0.0706       0.0902       0.0804       0.0947        0.123        0.109       0.0342      0.00285\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train               9 1198.871    0.005     0.000933     3.65e-05     0.000969       0.0787        0.105       0.0756        0.085       0.0803        0.101        0.112        0.106        0.195       0.0162\n",
      "! Validation          9 1198.871    0.005     0.000726     8.73e-07     0.000726       0.0686       0.0922       0.0637       0.0783        0.071       0.0855        0.104       0.0949       0.0291      0.00242\n",
      "Wall time: 1198.8723017920274\n",
      "! Best model        9    0.001\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     10    10      0.00161      0.00145     0.000158       0.0975         0.13       0.0835        0.126        0.105        0.111        0.162        0.137        0.516        0.043\n",
      "     10    20     0.000594     0.000576     1.76e-05        0.064       0.0822       0.0647       0.0625       0.0636       0.0835       0.0795       0.0815         0.17       0.0142\n",
      "     10    30     0.000774      0.00077     3.13e-06       0.0738        0.095       0.0665       0.0885       0.0775       0.0843        0.113       0.0989       0.0604      0.00503\n",
      "     10    40     0.000538     0.000537     4.01e-07       0.0613       0.0793       0.0562       0.0715       0.0638       0.0744       0.0884       0.0814       0.0196      0.00163\n",
      "     10    50     0.000878     0.000875     2.66e-06       0.0751        0.101       0.0701       0.0853       0.0777       0.0899        0.121        0.105       0.0604      0.00504\n",
      "     10    60     0.000877     0.000868     9.01e-06       0.0789        0.101       0.0759       0.0848       0.0803       0.0949        0.112        0.103        0.122       0.0101\n",
      "     10    70     0.000817     0.000756     6.08e-05       0.0723       0.0941       0.0731       0.0707       0.0719       0.0945       0.0932       0.0939        0.317       0.0264\n",
      "     10    80     0.000493     0.000493     1.09e-07       0.0615        0.076       0.0582       0.0682       0.0632       0.0734       0.0808       0.0771       0.0125      0.00105\n",
      "     10    90     0.000529     0.000517     1.26e-05       0.0627       0.0778       0.0636        0.061       0.0623       0.0795       0.0744       0.0769        0.145       0.0121\n",
      "     10   100     0.000661     0.000642     1.84e-05       0.0634       0.0867       0.0592       0.0717       0.0654       0.0799       0.0989       0.0894        0.174       0.0145\n",
      "     10   110     0.000491     0.000486     5.73e-06       0.0601       0.0754        0.062       0.0562       0.0591       0.0777       0.0706       0.0742        0.097      0.00808\n",
      "     10   120     0.000727     0.000726     1.33e-06       0.0705       0.0922       0.0662       0.0791       0.0727       0.0879          0.1       0.0941       0.0348       0.0029\n",
      "     10   130      0.00113      0.00112     1.15e-05       0.0742        0.114       0.0662       0.0903       0.0783       0.0973        0.143         0.12        0.126       0.0105\n",
      "     10   140      0.00109     0.000916     0.000173       0.0823        0.104       0.0792       0.0886       0.0839        0.102        0.107        0.105        0.538       0.0448\n",
      "     10   150     0.000734     0.000558     0.000177       0.0631       0.0808       0.0603       0.0685       0.0644       0.0784       0.0854       0.0819        0.545       0.0454\n",
      "     10   160     0.000948     0.000947     1.44e-06       0.0851        0.105       0.0831       0.0891       0.0861        0.101        0.113        0.107       0.0468       0.0039\n",
      "     10   170      0.00197      0.00194     2.46e-05        0.103        0.151        0.108       0.0916          0.1        0.163        0.123        0.143        0.191       0.0159\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     10     5     0.000816     0.000815     1.06e-06       0.0724       0.0977       0.0673       0.0825       0.0749       0.0891        0.113        0.101       0.0299      0.00249\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              10 1332.266    0.005      0.00089     3.36e-05     0.000924       0.0768        0.102       0.0741        0.082       0.0781       0.0987        0.109        0.104        0.182       0.0152\n",
      "! Validation         10 1332.266    0.005     0.000632     7.93e-07     0.000633       0.0642       0.0861       0.0601       0.0723       0.0662       0.0803       0.0965       0.0884       0.0268      0.00223\n",
      "Wall time: 1332.2668219580082\n",
      "! Best model       10    0.001\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     11    10     0.000636     0.000605     3.07e-05       0.0649       0.0842        0.057       0.0806       0.0688       0.0758       0.0989       0.0874        0.227        0.019\n",
      "     11    20     0.000695     0.000673     2.24e-05       0.0691       0.0888       0.0678       0.0717       0.0698       0.0871        0.092       0.0896        0.193       0.0161\n",
      "     11    30     0.000944      0.00094     3.88e-06       0.0808        0.105       0.0769       0.0885       0.0827        0.102         0.11        0.106       0.0713      0.00594\n",
      "     11    40     0.000659     0.000653     6.09e-06       0.0667       0.0875       0.0642       0.0717       0.0679       0.0834       0.0952       0.0893       0.0926      0.00772\n",
      "     11    50     0.000771     0.000767     4.74e-06       0.0652       0.0948       0.0669       0.0619       0.0644        0.102       0.0786       0.0903        0.086      0.00716\n",
      "     11    60     0.000403     0.000344     5.92e-05       0.0506       0.0634       0.0509       0.0498       0.0504       0.0643       0.0616        0.063        0.315       0.0262\n",
      "     11    70     0.000685     0.000675     1.02e-05       0.0672       0.0889        0.071       0.0595       0.0653       0.0953       0.0744       0.0849        0.121       0.0101\n",
      "     11    80      0.00103     0.000927     0.000104       0.0809        0.104       0.0802       0.0822       0.0812        0.105        0.103        0.104        0.416       0.0346\n",
      "     11    90     0.000856     0.000845     1.12e-05       0.0791       0.0995         0.08       0.0772       0.0786        0.101       0.0954       0.0984        0.135       0.0112\n",
      "     11   100      0.00066     0.000652     8.63e-06       0.0674       0.0874       0.0679       0.0662       0.0671       0.0863       0.0894       0.0879        0.115      0.00954\n",
      "     11   110      0.00103     0.000902     0.000126       0.0814        0.103       0.0775        0.089       0.0833       0.0969        0.114        0.105        0.458       0.0382\n",
      "     11   120     0.000517     0.000493      2.4e-05       0.0601        0.076       0.0594       0.0614       0.0604       0.0755       0.0769       0.0762          0.2       0.0166\n",
      "     11   130     0.000441     0.000394     4.66e-05       0.0531       0.0679         0.05       0.0593       0.0547       0.0634       0.0763       0.0698         0.28       0.0233\n",
      "     11   140     0.000752     0.000751     1.45e-06        0.074       0.0938       0.0667       0.0885       0.0776       0.0827        0.113       0.0977       0.0442      0.00368\n",
      "     11   150     0.000527     0.000526     1.35e-06        0.062       0.0785       0.0609       0.0642       0.0625       0.0757       0.0838       0.0798        0.029      0.00242\n",
      "     11   160     0.000618      0.00059     2.84e-05       0.0643       0.0831       0.0662       0.0606       0.0634       0.0854       0.0784       0.0819        0.217        0.018\n",
      "     11   170     0.000838     0.000837     6.19e-07       0.0762        0.099       0.0686       0.0915         0.08       0.0868         0.12        0.103       0.0287      0.00239\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     11     5     0.000672     0.000671     1.06e-06       0.0661       0.0887       0.0615       0.0753       0.0684       0.0808        0.103       0.0917        0.027      0.00225\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              11 1466.494    0.005     0.000738     3.06e-05     0.000769       0.0705        0.093       0.0684       0.0748       0.0716       0.0903       0.0982       0.0942        0.179       0.0149\n",
      "! Validation         11 1466.494    0.005     0.000559     7.74e-07      0.00056       0.0607       0.0809       0.0569       0.0682       0.0625       0.0757       0.0904       0.0831       0.0261      0.00217\n",
      "Wall time: 1466.4950677499874\n",
      "! Best model       11    0.001\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     12    10     0.000338     0.000336     1.56e-06       0.0504       0.0628       0.0496       0.0521       0.0508       0.0608       0.0665       0.0636       0.0502      0.00419\n",
      "     12    20     0.000607     0.000604     2.97e-06       0.0665       0.0841       0.0656       0.0681       0.0669       0.0847       0.0829       0.0838       0.0492       0.0041\n",
      "     12    30     0.000575     0.000566     9.55e-06       0.0648       0.0814         0.06       0.0742       0.0671       0.0751       0.0927       0.0839        0.125       0.0104\n",
      "     12    40     0.000723      0.00072     2.41e-06       0.0708       0.0919       0.0656       0.0814       0.0735       0.0869        0.101        0.094       0.0605      0.00504\n",
      "     12    50     0.000995     0.000981      1.4e-05       0.0753        0.107       0.0773       0.0711       0.0742        0.112       0.0975        0.105        0.145       0.0121\n",
      "     12    60     0.000815     0.000764     5.07e-05       0.0694       0.0946       0.0776        0.053       0.0653        0.105       0.0679       0.0867        0.282       0.0235\n",
      "     12    70     0.000813     0.000741     7.17e-05       0.0745       0.0932         0.07       0.0834       0.0767       0.0845        0.108       0.0965        0.346       0.0288\n",
      "     12    80     0.000584     0.000577     6.49e-06        0.064       0.0822       0.0604       0.0713       0.0658       0.0775        0.091       0.0842        0.103      0.00859\n",
      "     12    90     0.000761     0.000701     6.02e-05       0.0706       0.0906        0.069       0.0737       0.0714       0.0892       0.0935       0.0913        0.317       0.0264\n",
      "     12   100     0.000704     0.000688     1.55e-05       0.0692       0.0898       0.0699       0.0679       0.0689       0.0894       0.0906         0.09        0.159       0.0133\n",
      "     12   110      0.00111       0.0011     1.28e-05       0.0853        0.113       0.0843       0.0871       0.0857        0.115        0.109        0.112        0.143       0.0119\n",
      "     12   120      0.00136      0.00131     4.44e-05       0.0968        0.124       0.0936        0.103       0.0984        0.121        0.129        0.125        0.268       0.0223\n",
      "     12   130     0.000971     0.000788     0.000182       0.0767       0.0961       0.0796        0.071       0.0753       0.0988       0.0904       0.0946        0.555       0.0462\n",
      "     12   140     0.000683     0.000623     6.01e-05       0.0671       0.0854       0.0654       0.0704       0.0679       0.0853       0.0857       0.0855        0.318       0.0265\n",
      "     12   150      0.00132      0.00131     8.27e-07       0.0904        0.124       0.0954       0.0803       0.0878        0.132        0.108         0.12       0.0288       0.0024\n",
      "     12   160     0.000574      0.00056     1.36e-05       0.0651        0.081       0.0642        0.067       0.0656       0.0808       0.0813       0.0811        0.149       0.0124\n",
      "     12   170     0.000437     0.000434     2.91e-06       0.0541       0.0713       0.0535       0.0554       0.0545       0.0696       0.0747       0.0721       0.0571      0.00476\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     12     5     0.000614     0.000613     8.41e-07       0.0632       0.0847       0.0584       0.0729       0.0657       0.0764       0.0993       0.0879       0.0233      0.00194\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              12 1605.877    0.005     0.000689      2.9e-05     0.000718       0.0682       0.0898       0.0661       0.0726       0.0693       0.0871        0.095       0.0911        0.174       0.0145\n",
      "! Validation         12 1605.877    0.005     0.000501     6.37e-07     0.000501       0.0576       0.0766       0.0543       0.0643       0.0593       0.0718       0.0854       0.0786       0.0237      0.00197\n",
      "Wall time: 1605.8780931669753\n",
      "! Best model       12    0.001\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     13    10     0.000784     0.000782     2.12e-06         0.07       0.0957       0.0691       0.0719       0.0705        0.096       0.0952       0.0956       0.0582      0.00485\n",
      "     13    20     0.000483     0.000459     2.44e-05       0.0557       0.0733       0.0541        0.059       0.0565       0.0705       0.0786       0.0745        0.202       0.0168\n",
      "     13    30     0.000411      0.00041        4e-07       0.0522       0.0693       0.0473       0.0619       0.0546        0.063       0.0805       0.0718        0.023      0.00191\n",
      "     13    40     0.000332     0.000331     5.25e-07       0.0492       0.0623         0.05       0.0475       0.0487       0.0632       0.0604       0.0618       0.0225      0.00188\n",
      "     13    50     0.000476     0.000416     5.92e-05       0.0562       0.0698        0.051       0.0667       0.0588       0.0633       0.0814       0.0723        0.316       0.0263\n",
      "     13    60     0.000417     0.000417     8.54e-07       0.0565       0.0699       0.0504       0.0687       0.0595       0.0631       0.0817       0.0724        0.035      0.00292\n",
      "     13    70      0.00105     0.000955     9.78e-05       0.0823        0.106       0.0768       0.0932        0.085       0.0937        0.127         0.11        0.406       0.0338\n",
      "     13    80     0.000557     0.000553     4.41e-06       0.0599       0.0805       0.0557       0.0682        0.062       0.0761       0.0885       0.0823       0.0727      0.00606\n",
      "     13    90     0.000906     0.000875     3.06e-05       0.0777        0.101        0.074       0.0852       0.0796        0.101        0.102        0.101        0.221       0.0184\n",
      "     13   100     0.000602     0.000598     3.57e-06        0.066       0.0837       0.0601       0.0779        0.069       0.0734        0.101       0.0873       0.0762      0.00635\n",
      "     13   110     0.000554     0.000535     1.93e-05        0.058       0.0791       0.0538       0.0663       0.0601       0.0768       0.0836       0.0802         0.18        0.015\n",
      "     13   120     0.000824      0.00081     1.37e-05       0.0724       0.0974       0.0653       0.0866       0.0759       0.0859        0.117        0.102        0.151       0.0125\n",
      "     13   130      0.00103     0.000974     5.42e-05       0.0857        0.107       0.0786       0.0998       0.0892       0.0975        0.123         0.11          0.3        0.025\n",
      "     13   140     0.000828      0.00081     1.79e-05       0.0778       0.0974       0.0718       0.0898       0.0808       0.0911        0.109          0.1        0.172       0.0143\n",
      "     13   150     0.000735     0.000715     1.97e-05       0.0744       0.0915        0.072       0.0793       0.0756       0.0889       0.0966       0.0927        0.179       0.0149\n",
      "     13   160      0.00061     0.000597     1.25e-05       0.0612       0.0836       0.0546       0.0743       0.0644       0.0741          0.1       0.0871         0.14       0.0117\n",
      "     13   170     0.000793     0.000786     7.24e-06       0.0733       0.0959       0.0679        0.084        0.076       0.0897        0.107       0.0985       0.0849      0.00708\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     13     5     0.000565     0.000565      7.9e-07       0.0608       0.0813       0.0557        0.071       0.0633       0.0726       0.0964       0.0845       0.0205       0.0017\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              13 1740.978    0.005     0.000716     2.14e-05     0.000737       0.0695       0.0916       0.0673        0.074       0.0707       0.0884       0.0976        0.093        0.154       0.0128\n",
      "! Validation         13 1740.978    0.005     0.000464     6.16e-07     0.000464       0.0558       0.0737       0.0524       0.0625       0.0575       0.0691       0.0822       0.0756       0.0227      0.00189\n",
      "Wall time: 1740.9790621669963\n",
      "! Best model       13    0.000\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     14    10      0.00178      0.00177     7.34e-06        0.109        0.144        0.099        0.129        0.114        0.132        0.165        0.149        0.103      0.00857\n",
      "     14    20      0.00189      0.00127      0.00062       0.0935        0.122       0.0929       0.0946       0.0938        0.121        0.123        0.122         1.02        0.085\n",
      "     14    30      0.00135       0.0012      0.00015       0.0927        0.119       0.0877        0.103       0.0952         0.11        0.135        0.122        0.501       0.0418\n",
      "     14    40      0.00099     0.000967     2.28e-05       0.0818        0.106       0.0756       0.0943        0.085       0.0975        0.122         0.11        0.191       0.0159\n",
      "     14    50      0.00144      0.00133      0.00011       0.0995        0.125       0.0939        0.111        0.102        0.122         0.13        0.126        0.425       0.0354\n",
      "     14    60        0.001     0.000906     9.63e-05       0.0824        0.103       0.0793       0.0885       0.0839       0.0965        0.115        0.106        0.402       0.0335\n",
      "     14    70     0.000636     0.000619     1.63e-05        0.067       0.0852       0.0644       0.0723       0.0684       0.0805       0.0939       0.0872        0.164       0.0136\n",
      "     14    80      0.00109      0.00107     1.73e-05       0.0817        0.112       0.0782       0.0886       0.0834        0.104        0.127        0.115         0.16       0.0133\n",
      "     14    90     0.000484     0.000484     3.91e-07       0.0589       0.0753       0.0583       0.0602       0.0593       0.0731       0.0794       0.0763       0.0235      0.00196\n",
      "     14   100     0.000986     0.000969     1.69e-05       0.0792        0.107       0.0817       0.0741       0.0779        0.111       0.0975        0.104        0.166       0.0138\n",
      "     14   110     0.000398     0.000397     8.22e-07       0.0525       0.0682       0.0514       0.0547       0.0531       0.0661       0.0721       0.0691       0.0288       0.0024\n",
      "     14   120      0.00055     0.000549     2.34e-07       0.0651       0.0802        0.066       0.0633       0.0646       0.0817       0.0772       0.0795       0.0172      0.00143\n",
      "     14   130     0.000681     0.000609     7.18e-05       0.0649       0.0845       0.0656       0.0635       0.0646       0.0856       0.0822       0.0839        0.347       0.0289\n",
      "     14   140     0.000714      0.00067     4.36e-05        0.066       0.0886       0.0581       0.0819         0.07       0.0756         0.11       0.0929        0.267       0.0222\n",
      "     14   150      0.00037     0.000367     3.39e-06       0.0518       0.0656       0.0506       0.0543       0.0524       0.0638        0.069       0.0664       0.0742      0.00619\n",
      "     14   160     0.000192     0.000191     9.12e-07       0.0365       0.0473       0.0367       0.0361       0.0364        0.046       0.0496       0.0478       0.0247      0.00206\n",
      "     14   170     0.000576     0.000575     9.23e-07       0.0677       0.0821       0.0687       0.0657       0.0672       0.0841       0.0778        0.081        0.033      0.00275\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     14     5     0.000545     0.000544     9.95e-07       0.0594       0.0798       0.0548       0.0685       0.0617       0.0721       0.0934       0.0828       0.0242      0.00202\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              14 1876.468    0.005      0.00084     5.06e-05      0.00089       0.0728       0.0992         0.07       0.0783       0.0742       0.0946        0.108        0.101        0.214       0.0178\n",
      "! Validation         14 1876.468    0.005     0.000445     6.72e-07     0.000445       0.0549       0.0722       0.0518       0.0612       0.0565       0.0681       0.0798       0.0739       0.0237      0.00197\n",
      "Wall time: 1876.4687520420412\n",
      "! Best model       14    0.000\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     15    10     0.000406     0.000377     2.92e-05       0.0509       0.0664       0.0482       0.0564       0.0523       0.0624       0.0738       0.0681         0.22       0.0184\n",
      "     15    20     0.000529     0.000527     1.66e-06       0.0631       0.0786       0.0618       0.0657       0.0637       0.0776       0.0805       0.0791       0.0463      0.00386\n",
      "     15    30     0.000624     0.000584     3.98e-05       0.0668       0.0827       0.0641       0.0721       0.0681       0.0782        0.091       0.0846        0.256       0.0213\n",
      "     15    40     0.000483     0.000473     1.04e-05       0.0588       0.0744       0.0589       0.0587       0.0588       0.0735       0.0762       0.0749         0.13       0.0109\n",
      "     15    50     0.000566     0.000558     7.71e-06       0.0631       0.0808       0.0596       0.0703       0.0649       0.0774       0.0874       0.0824        0.111      0.00923\n",
      "     15    60     0.000481     0.000439     4.27e-05       0.0502       0.0717       0.0476       0.0555       0.0516       0.0687       0.0774        0.073        0.266       0.0222\n",
      "     15    70     0.000437     0.000395     4.26e-05       0.0547        0.068       0.0499       0.0643       0.0571       0.0627       0.0775       0.0701        0.267       0.0222\n",
      "     15    80     0.000586     0.000562      2.4e-05       0.0649       0.0812       0.0692       0.0562       0.0627       0.0837       0.0758       0.0797        0.199       0.0166\n",
      "     15    90     0.000436     0.000431     4.95e-06       0.0529       0.0711       0.0558       0.0471       0.0515       0.0762       0.0594       0.0678       0.0863       0.0072\n",
      "     15   100     0.000564     0.000558     6.47e-06       0.0606       0.0808       0.0554       0.0712       0.0633       0.0712       0.0973       0.0842        0.102      0.00853\n",
      "     15   110     0.000694     0.000687     6.49e-06        0.072       0.0897       0.0701       0.0758       0.0729       0.0897       0.0899       0.0898        0.103      0.00859\n",
      "     15   120     0.000999     0.000996     3.41e-06       0.0784        0.108       0.0801        0.075       0.0776        0.114       0.0956        0.105       0.0622      0.00519\n",
      "     15   130     0.000515     0.000508     7.03e-06        0.062       0.0772       0.0663       0.0533       0.0598       0.0831       0.0637       0.0734        0.103      0.00856\n",
      "     15   140     0.000837     0.000784     5.38e-05       0.0706       0.0958       0.0764       0.0591       0.0677        0.101       0.0841       0.0926          0.3        0.025\n",
      "     15   150     0.000296     0.000295      5.3e-07       0.0466       0.0588       0.0456       0.0485       0.0471       0.0567       0.0628       0.0597       0.0242      0.00202\n",
      "     15   160     0.000507     0.000499     7.48e-06       0.0622       0.0765       0.0617       0.0631       0.0624       0.0756       0.0782       0.0769        0.111      0.00925\n",
      "     15   170     0.000373     0.000355     1.82e-05       0.0518       0.0645       0.0516       0.0522       0.0519       0.0636       0.0662       0.0649        0.172       0.0144\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     15     5      0.00049     0.000489     7.54e-07       0.0567       0.0757       0.0513       0.0675       0.0594       0.0676       0.0898       0.0787       0.0224      0.00186\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              15 2020.860    0.005     0.000583     2.13e-05     0.000604       0.0629       0.0826       0.0608       0.0672        0.064         0.08       0.0877       0.0838        0.158       0.0132\n",
      "! Validation         15 2020.860    0.005     0.000414     5.61e-07     0.000414        0.053       0.0696       0.0499       0.0592       0.0545       0.0656       0.0771       0.0713       0.0223      0.00186\n",
      "Wall time: 2020.860644957982\n",
      "! Best model       15    0.000\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     16    10     0.000423     0.000422     9.07e-07       0.0527       0.0703       0.0515       0.0549       0.0532       0.0677       0.0753       0.0715       0.0352      0.00293\n",
      "     16    20     0.000415     0.000413     1.96e-06       0.0553       0.0696       0.0507       0.0644       0.0576       0.0641       0.0794       0.0718       0.0513      0.00427\n",
      "     16    30     0.000372     0.000363     9.59e-06       0.0513       0.0652       0.0529       0.0482       0.0505       0.0663       0.0628       0.0646        0.125       0.0104\n",
      "     16    40     0.000307     0.000305      2.4e-06       0.0461       0.0598       0.0477       0.0428       0.0453       0.0609       0.0575       0.0592        0.063      0.00525\n",
      "     16    50     0.000713     0.000712     1.79e-06       0.0712       0.0913       0.0756       0.0623        0.069       0.0967       0.0794       0.0881       0.0485      0.00404\n",
      "     16    60     0.000411     0.000392     1.87e-05       0.0544       0.0678       0.0507       0.0618       0.0562       0.0623       0.0775       0.0699        0.177       0.0148\n",
      "     16    70     0.000418     0.000411     6.82e-06       0.0542       0.0694       0.0532       0.0561       0.0547       0.0681       0.0719         0.07        0.106       0.0088\n",
      "     16    80     0.000288     0.000273     1.56e-05       0.0432       0.0565        0.039       0.0517       0.0453        0.053        0.063        0.058        0.161       0.0134\n",
      "     16    90     0.000362     0.000339     2.27e-05       0.0481       0.0631       0.0445       0.0554       0.0499       0.0589       0.0706       0.0648        0.194       0.0161\n",
      "     16   100     0.000484     0.000479     4.98e-06       0.0618       0.0749       0.0629       0.0595       0.0612       0.0769       0.0709       0.0739       0.0821      0.00684\n",
      "     16   110     0.000629      0.00048     0.000149       0.0596        0.075       0.0575       0.0639       0.0607       0.0735        0.078       0.0758        0.501       0.0417\n",
      "     16   120     0.000466     0.000457     9.39e-06       0.0559       0.0732       0.0581       0.0516       0.0549       0.0765       0.0659       0.0712        0.123       0.0102\n",
      "     16   130     0.000608     0.000581     2.68e-05       0.0649       0.0825       0.0667       0.0614        0.064       0.0844       0.0786       0.0815        0.211       0.0176\n",
      "     16   140     0.000466     0.000458     8.51e-06       0.0579       0.0732       0.0554        0.063       0.0592       0.0684       0.0819       0.0752        0.115      0.00961\n",
      "     16   150      0.00107      0.00101     5.95e-05       0.0857        0.109       0.0799       0.0974       0.0886       0.0984        0.127        0.113        0.315       0.0263\n",
      "     16   160      0.00102      0.00102     1.29e-06       0.0717        0.109       0.0666       0.0819       0.0742       0.0961        0.131        0.114       0.0427      0.00356\n",
      "     16   170     0.000721     0.000679      4.2e-05       0.0702       0.0892       0.0663       0.0779       0.0721       0.0863       0.0946       0.0905        0.264        0.022\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     16     5     0.000475     0.000475     6.76e-07       0.0555       0.0746       0.0503        0.066       0.0581       0.0668       0.0881       0.0774       0.0223      0.00186\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              16 2172.936    0.005     0.000513     1.96e-05     0.000532        0.059       0.0775       0.0575        0.062       0.0598       0.0755       0.0813       0.0784        0.142       0.0119\n",
      "! Validation         16 2172.936    0.005     0.000387     5.14e-07     0.000388       0.0511       0.0674       0.0481        0.057       0.0525       0.0636       0.0743        0.069       0.0215      0.00179\n",
      "Wall time: 2172.9365592079703\n",
      "! Best model       16    0.000\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     17    10     0.000373     0.000345     2.78e-05       0.0491       0.0636       0.0505       0.0462       0.0483       0.0672       0.0557       0.0614        0.215       0.0179\n",
      "     17    20      0.00047     0.000466     3.85e-06       0.0565       0.0739       0.0555       0.0586        0.057       0.0697       0.0817       0.0757       0.0766      0.00638\n",
      "     17    30      0.00057     0.000549     2.13e-05       0.0627       0.0802       0.0635       0.0612       0.0624       0.0813       0.0779       0.0796        0.182       0.0151\n",
      "     17    40     0.000345     0.000337     7.95e-06       0.0487       0.0628       0.0465        0.053       0.0498       0.0593       0.0692       0.0643        0.111      0.00923\n",
      "     17    50     0.000991     0.000973     1.85e-05       0.0795        0.107       0.0768       0.0849       0.0809        0.105         0.11        0.108        0.172       0.0144\n",
      "     17    60      0.00102        0.001     2.11e-05       0.0836        0.108       0.0744        0.102       0.0883       0.0971        0.128        0.113        0.185       0.0154\n",
      "     17    70      0.00104      0.00102     1.79e-05        0.083         0.11       0.0866       0.0758       0.0812        0.112        0.104        0.108         0.16       0.0133\n",
      "     17    80      0.00188      0.00179     8.23e-05        0.113        0.145        0.105        0.128        0.116        0.138        0.158        0.148        0.372        0.031\n",
      "     17    90     0.000799      0.00075     4.94e-05       0.0758       0.0937       0.0727       0.0818       0.0773       0.0895        0.102       0.0956        0.286       0.0238\n",
      "     17   100     0.000755     0.000735     1.99e-05        0.071       0.0928       0.0675        0.078       0.0728       0.0868        0.104       0.0953        0.176       0.0146\n",
      "     17   110      0.00049     0.000481     9.38e-06       0.0582       0.0751       0.0523       0.0701       0.0612        0.068       0.0875       0.0777        0.124       0.0104\n",
      "     17   120     0.000874     0.000864     9.91e-06       0.0776        0.101       0.0785        0.076       0.0772        0.102        0.098       0.0999        0.125       0.0104\n",
      "     17   130     0.000475     0.000327     0.000148        0.049       0.0619       0.0474       0.0521       0.0498       0.0608        0.064       0.0624        0.499       0.0416\n",
      "     17   140     0.000759     0.000757     2.26e-06       0.0605       0.0941       0.0574       0.0668       0.0621       0.0886        0.104       0.0965       0.0556      0.00464\n",
      "     17   150     0.000696     0.000688     8.54e-06       0.0666       0.0898       0.0649         0.07       0.0675       0.0877       0.0937       0.0907        0.115      0.00957\n",
      "     17   160     0.000951     0.000942     8.79e-06       0.0831        0.105       0.0811       0.0871       0.0841        0.105        0.105        0.105        0.119      0.00991\n",
      "     17   170     0.000525     0.000481     4.36e-05       0.0603       0.0751       0.0583       0.0643       0.0613       0.0742       0.0768       0.0755         0.27       0.0225\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     17     5     0.000417     0.000416     6.61e-07       0.0531       0.0698       0.0481       0.0631       0.0556       0.0632       0.0816       0.0724       0.0215      0.00179\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              17 2318.762    0.005     0.000782     2.24e-05     0.000804       0.0694       0.0957       0.0661        0.076        0.071       0.0905        0.105       0.0979        0.155       0.0129\n",
      "! Validation         17 2318.762    0.005     0.000364     5.12e-07     0.000365       0.0499       0.0653       0.0474        0.055       0.0512       0.0622       0.0711       0.0667       0.0214      0.00178\n",
      "Wall time: 2318.7632178750355\n",
      "! Best model       17    0.000\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     18    10     0.000547     0.000541     5.51e-06       0.0641       0.0796       0.0625       0.0672       0.0649       0.0776       0.0835       0.0805       0.0911      0.00759\n",
      "     18    20     0.000345     0.000305     3.97e-05       0.0471       0.0598       0.0465       0.0484       0.0475       0.0595       0.0603       0.0599        0.258       0.0215\n",
      "     18    30     0.000634     0.000632     2.05e-06       0.0618       0.0861       0.0565       0.0723       0.0644       0.0819       0.0939       0.0879       0.0516       0.0043\n",
      "     18    40     0.000751      0.00069     6.11e-05       0.0688       0.0899       0.0605       0.0855        0.073       0.0813        0.105       0.0932         0.32       0.0266\n",
      "     18    50     0.000746     0.000734     1.26e-05       0.0705       0.0927       0.0689       0.0736       0.0713       0.0892       0.0993       0.0943        0.144        0.012\n",
      "     18    60     0.000453     0.000441     1.13e-05       0.0547       0.0719        0.048       0.0681       0.0581       0.0629       0.0872        0.075        0.135       0.0113\n",
      "     18    70     0.000851     0.000797     5.38e-05       0.0752       0.0967       0.0698       0.0862        0.078       0.0911        0.107        0.099        0.297       0.0248\n",
      "     18    80     0.000801     0.000728     7.25e-05       0.0663       0.0924        0.073       0.0531        0.063        0.101       0.0713       0.0863        0.347       0.0289\n",
      "     18    90     0.000826     0.000805     2.12e-05       0.0754       0.0971       0.0715       0.0831       0.0773       0.0928        0.105        0.099        0.187       0.0156\n",
      "     18   100     0.000448     0.000351     9.68e-05       0.0513       0.0642       0.0518       0.0502        0.051       0.0656       0.0611       0.0634        0.404       0.0336\n",
      "     18   110     0.000605     0.000599     6.51e-06       0.0662       0.0838       0.0639       0.0709       0.0674       0.0803       0.0904       0.0853        0.101      0.00845\n",
      "     18   120     0.000473     0.000444     2.83e-05       0.0565       0.0721       0.0498       0.0699       0.0599        0.062        0.089       0.0755        0.217       0.0181\n",
      "     18   130     0.000513     0.000487     2.55e-05       0.0567       0.0756       0.0591       0.0517       0.0554       0.0803        0.065       0.0727        0.205       0.0171\n",
      "     18   140     0.000317     0.000295     2.16e-05       0.0468       0.0588       0.0466        0.047       0.0468        0.059       0.0584       0.0587         0.19       0.0159\n",
      "     18   150     0.000332     0.000294     3.78e-05       0.0459       0.0587       0.0429       0.0519       0.0474       0.0555       0.0646       0.0601        0.252        0.021\n",
      "     18   160      0.00061     0.000601     9.01e-06       0.0644       0.0839       0.0644       0.0643       0.0644        0.083       0.0856       0.0843        0.116      0.00964\n",
      "     18   170     0.000612     0.000612     5.24e-07       0.0654       0.0847       0.0592       0.0778       0.0685       0.0754        0.101        0.088       0.0251      0.00209\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     18     5     0.000397     0.000396     6.73e-07       0.0518       0.0681       0.0475       0.0603       0.0539       0.0623       0.0784       0.0704       0.0229      0.00191\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              18 2487.379    0.005     0.000565     3.14e-05     0.000596       0.0609       0.0813       0.0588        0.065       0.0619       0.0783        0.087       0.0827        0.187       0.0156\n",
      "! Validation         18 2487.379    0.005     0.000344     4.87e-07     0.000345       0.0486       0.0635       0.0461       0.0534       0.0498       0.0609       0.0685       0.0647       0.0212      0.00177\n",
      "Wall time: 2487.379720582976\n",
      "! Best model       18    0.000\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     19    10     0.000444     0.000443     9.37e-07       0.0561        0.072       0.0549       0.0584       0.0566       0.0721       0.0719        0.072       0.0371      0.00309\n",
      "     19    20     0.000482      0.00048     2.36e-06       0.0565        0.075        0.055       0.0594       0.0572       0.0713       0.0819       0.0766       0.0618      0.00515\n",
      "     19    30     0.000465      0.00044     2.46e-05       0.0542       0.0718       0.0575       0.0476       0.0526       0.0742       0.0669       0.0705        0.203       0.0169\n",
      "     19    40     0.000304     0.000287     1.62e-05       0.0432        0.058       0.0455       0.0386        0.042       0.0605       0.0527       0.0566        0.164       0.0137\n",
      "     19    50     0.000198     0.000191      6.2e-06       0.0375       0.0474       0.0349       0.0428       0.0388       0.0442        0.053       0.0486       0.0981      0.00817\n",
      "     19    60     0.000426     0.000422     3.73e-06       0.0568       0.0703        0.058       0.0543       0.0562       0.0717       0.0676       0.0696       0.0777      0.00647\n",
      "     19    70     0.000549     0.000544     5.01e-06       0.0631       0.0798       0.0626       0.0641       0.0633       0.0789       0.0815       0.0802       0.0867      0.00722\n",
      "     19    80      0.00046     0.000425     3.45e-05       0.0537       0.0706       0.0495        0.062       0.0557       0.0631       0.0836       0.0733         0.24         0.02\n",
      "     19    90      0.00054     0.000532     8.36e-06        0.063       0.0789       0.0628       0.0634       0.0631       0.0774        0.082       0.0797        0.115      0.00959\n",
      "     19   100     0.000476     0.000474     2.68e-06       0.0569       0.0745       0.0575       0.0555       0.0565       0.0745       0.0745       0.0745       0.0619      0.00516\n",
      "     19   110     0.000534     0.000532     1.86e-06        0.061       0.0789        0.055        0.073        0.064        0.074        0.088        0.081       0.0496      0.00414\n",
      "     19   120     0.000512     0.000502     9.98e-06       0.0596       0.0767       0.0559       0.0669       0.0614        0.072       0.0854       0.0787        0.124       0.0103\n",
      "     19   130     0.000486     0.000483     2.61e-06       0.0531       0.0752        0.047       0.0652       0.0561       0.0647       0.0928       0.0788       0.0567      0.00473\n",
      "     19   140     0.000265     0.000256     8.54e-06       0.0411       0.0548       0.0422       0.0388       0.0405       0.0556       0.0531       0.0544        0.118      0.00983\n",
      "     19   150     0.000325      0.00032     4.64e-06       0.0476       0.0612       0.0436       0.0556       0.0496        0.058       0.0671       0.0626       0.0851       0.0071\n",
      "     19   160     0.000307     0.000306     8.62e-07       0.0463       0.0598       0.0449        0.049        0.047       0.0582       0.0629       0.0606       0.0359      0.00299\n",
      "     19   170     0.000328     0.000326     1.23e-06       0.0504       0.0618       0.0496       0.0522       0.0509       0.0602        0.065       0.0626       0.0382      0.00319\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     19     5     0.000365     0.000365     6.02e-07       0.0501       0.0654       0.0454       0.0594       0.0524       0.0598       0.0753       0.0675       0.0223      0.00186\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              19 2643.282    0.005     0.000428     9.67e-06     0.000437       0.0543       0.0708       0.0527       0.0575       0.0551       0.0687       0.0747       0.0717        0.104      0.00867\n",
      "! Validation         19 2643.282    0.005     0.000327     4.33e-07     0.000328       0.0476       0.0619       0.0452       0.0523       0.0488       0.0594       0.0667        0.063       0.0203      0.00169\n",
      "Wall time: 2643.283040542039\n",
      "! Best model       19    0.000\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     20    10     0.000255      0.00016     9.54e-05       0.0345       0.0433       0.0356       0.0324        0.034       0.0446       0.0406       0.0426        0.401       0.0334\n",
      "     20    20     0.000963     0.000963     1.21e-07       0.0738        0.106       0.0767       0.0679       0.0723        0.106        0.107        0.106       0.0124      0.00103\n",
      "     20    30     0.000556     0.000554     1.35e-06       0.0639       0.0806       0.0649       0.0618       0.0634       0.0808       0.0801       0.0805       0.0386      0.00322\n",
      "     20    40     0.000668     0.000657     1.09e-05       0.0691       0.0878       0.0622       0.0831       0.0726       0.0763        0.107       0.0917        0.135       0.0112\n",
      "     20    50     0.000425      0.00042     5.18e-06       0.0555       0.0701       0.0544       0.0577        0.056        0.069       0.0724       0.0707        0.091      0.00758\n",
      "     20    60     0.000315     0.000274      4.1e-05       0.0461       0.0567       0.0473       0.0438       0.0456       0.0573       0.0553       0.0563        0.263       0.0219\n",
      "     20    70     0.000778     0.000776      1.8e-06       0.0745       0.0953       0.0731       0.0771       0.0751       0.0953       0.0953       0.0953       0.0491      0.00409\n",
      "     20    80     0.000619     0.000617     2.25e-06       0.0686        0.085       0.0764       0.0529       0.0647       0.0931        0.066       0.0795       0.0498      0.00415\n",
      "     20    90     0.000613     0.000437     0.000177       0.0579       0.0715       0.0585       0.0567       0.0576       0.0727       0.0691       0.0709        0.545       0.0454\n",
      "     20   100      0.00024     0.000237     3.61e-06       0.0432       0.0527       0.0444       0.0407       0.0425       0.0544       0.0489       0.0517       0.0752      0.00626\n",
      "     20   110     0.000395     0.000392     2.28e-06       0.0511       0.0678       0.0498       0.0535       0.0517        0.067       0.0693       0.0682       0.0595      0.00496\n",
      "     20   120     0.000435     0.000425     9.56e-06       0.0557       0.0706       0.0524       0.0623       0.0573       0.0668       0.0776       0.0722        0.124       0.0103\n",
      "     20   130     0.000325     0.000325     5.36e-07       0.0494       0.0617       0.0464       0.0556        0.051       0.0577        0.069       0.0633       0.0247      0.00206\n",
      "     20   140     0.000335     0.000332     2.63e-06       0.0466       0.0624       0.0484       0.0429       0.0457       0.0647       0.0573        0.061       0.0612       0.0051\n",
      "     20   150     0.000845     0.000835     1.01e-05       0.0792       0.0989       0.0745       0.0887       0.0816       0.0915        0.112        0.102        0.129       0.0108\n",
      "     20   160     0.000371      0.00037     8.22e-07       0.0519       0.0658       0.0542       0.0473       0.0507       0.0678       0.0618       0.0648       0.0301      0.00251\n",
      "     20   170      0.00046     0.000449     1.15e-05       0.0562       0.0725        0.052       0.0648       0.0584       0.0667       0.0829       0.0748        0.134       0.0111\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     20     5     0.000363     0.000362     6.27e-07       0.0498       0.0651       0.0457        0.058       0.0519       0.0605       0.0736        0.067       0.0225      0.00187\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              20 2798.851    0.005     0.000472     2.35e-05     0.000495       0.0567       0.0743       0.0557       0.0588       0.0572        0.073       0.0769        0.075        0.158       0.0132\n",
      "! Validation         20 2798.851    0.005     0.000318     4.64e-07     0.000319       0.0469       0.0611       0.0449       0.0509       0.0479       0.0591       0.0649        0.062       0.0207      0.00172\n",
      "Wall time: 2798.8516313330038\n",
      "! Best model       20    0.000\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     21    10     0.000428     0.000427     5.85e-07        0.054       0.0707       0.0524       0.0571       0.0548       0.0678       0.0763        0.072       0.0285      0.00237\n",
      "     21    20     0.000303     0.000293     1.07e-05        0.045       0.0585       0.0444       0.0462       0.0453       0.0585       0.0587       0.0586        0.133       0.0111\n",
      "     21    30     0.000388     0.000374     1.45e-05       0.0531       0.0662       0.0515       0.0561       0.0538       0.0645       0.0695        0.067        0.155       0.0129\n",
      "     21    40     0.000282      0.00028     1.35e-06       0.0439       0.0573       0.0433       0.0452       0.0443       0.0561       0.0596       0.0579       0.0416      0.00347\n",
      "     21    50     0.000357     0.000353      4.4e-06       0.0511       0.0643       0.0539       0.0455       0.0497       0.0659        0.061       0.0634       0.0848      0.00707\n",
      "     21    60     0.000624     0.000622     2.92e-06       0.0683       0.0853       0.0729       0.0591        0.066       0.0911       0.0725       0.0818        0.051      0.00425\n",
      "     21    70     0.000359      0.00035     9.78e-06       0.0514        0.064       0.0512       0.0519       0.0515       0.0648       0.0624       0.0636        0.125       0.0104\n",
      "     21    80     0.000423     0.000355     6.83e-05        0.051       0.0645       0.0516       0.0498       0.0507       0.0654       0.0626        0.064        0.339       0.0282\n",
      "     21    90     0.000545     0.000544     1.03e-06       0.0615       0.0798       0.0588       0.0668       0.0628       0.0771        0.085        0.081       0.0343      0.00286\n",
      "     21   100     0.000424      0.00039     3.45e-05       0.0511       0.0676        0.046       0.0614       0.0537       0.0597       0.0811       0.0704         0.24         0.02\n",
      "     21   110     0.000334     0.000316     1.84e-05       0.0472       0.0608       0.0491       0.0434       0.0463       0.0622       0.0581       0.0601        0.176       0.0147\n",
      "     21   120     0.000337     0.000327     9.85e-06       0.0465       0.0619        0.046       0.0475       0.0467        0.062       0.0615       0.0618        0.125       0.0104\n",
      "     21   130     0.000328     0.000328     1.81e-07       0.0481        0.062       0.0461       0.0521       0.0491         0.06       0.0659       0.0629       0.0133      0.00111\n",
      "     21   140     0.000403     0.000402     1.84e-07       0.0543       0.0687       0.0531       0.0567       0.0549        0.068       0.0699        0.069       0.0129      0.00107\n",
      "     21   150     0.000327     0.000326     5.33e-07       0.0484       0.0618       0.0481        0.049       0.0486       0.0631        0.059       0.0611       0.0224      0.00187\n",
      "     21   160     0.000293     0.000284     9.15e-06       0.0456       0.0577        0.042       0.0527       0.0474       0.0534       0.0654       0.0594        0.122       0.0101\n",
      "     21   170     0.000465     0.000461     3.82e-06       0.0577       0.0735       0.0538       0.0655       0.0597       0.0681       0.0833       0.0757       0.0752      0.00626\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     21     5     0.000333     0.000332     5.94e-07       0.0479       0.0624       0.0443       0.0553       0.0498       0.0584       0.0698       0.0641       0.0227      0.00189\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              21 2966.955    0.005     0.000366     1.02e-05     0.000376       0.0502       0.0655       0.0493       0.0522       0.0507       0.0643       0.0678        0.066        0.101      0.00842\n",
      "! Validation         21 2966.955    0.005     0.000299     4.26e-07       0.0003       0.0457       0.0592        0.044       0.0493       0.0466       0.0575       0.0626         0.06       0.0201      0.00168\n",
      "Wall time: 2966.9552102499874\n",
      "! Best model       21    0.000\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     22    10     0.000418     0.000385      3.3e-05       0.0509       0.0672       0.0502       0.0522       0.0512       0.0663        0.069       0.0676        0.233       0.0194\n",
      "     22    20     0.000282     0.000266     1.54e-05        0.041       0.0559       0.0399       0.0432       0.0415       0.0539       0.0597       0.0568         0.16       0.0133\n",
      "     22    30     0.000348     0.000339     8.99e-06       0.0493        0.063       0.0501       0.0477       0.0489       0.0643       0.0602       0.0623        0.121       0.0101\n",
      "     22    40     0.000283      0.00027      1.3e-05       0.0446       0.0562       0.0447       0.0444       0.0446        0.057       0.0547       0.0558        0.147       0.0123\n",
      "     22    50     0.000292     0.000288     4.65e-06       0.0435       0.0581        0.041       0.0487       0.0448        0.054       0.0654       0.0597       0.0806      0.00672\n",
      "     22    60      0.00113      0.00108     5.09e-05       0.0808        0.113       0.0761       0.0902       0.0832        0.104        0.128        0.116        0.293       0.0244\n",
      "     22    70     0.000415     0.000382     3.38e-05       0.0541       0.0669       0.0561       0.0501       0.0531       0.0695       0.0613       0.0654        0.235       0.0196\n",
      "     22    80     0.000487     0.000465     2.27e-05       0.0569       0.0738       0.0586       0.0535        0.056       0.0761       0.0689       0.0725        0.192        0.016\n",
      "     22    90     0.000443     0.000391      5.2e-05       0.0524       0.0677       0.0441        0.069       0.0566       0.0596       0.0815       0.0705        0.295       0.0245\n",
      "     22   100     0.000454     0.000429     2.51e-05       0.0572       0.0709        0.056       0.0595       0.0577        0.071       0.0707       0.0708        0.204        0.017\n",
      "     22   110     0.000525     0.000483     4.19e-05       0.0606       0.0752       0.0638       0.0541        0.059       0.0794        0.066       0.0727        0.265       0.0221\n",
      "     22   120     0.000317     0.000315      1.3e-06       0.0474       0.0608       0.0457       0.0508       0.0482       0.0582       0.0657       0.0619        0.045      0.00375\n",
      "     22   130     0.000371      0.00037     8.52e-07       0.0515       0.0658       0.0523       0.0499       0.0511       0.0683       0.0606       0.0644       0.0296      0.00247\n",
      "     22   140     0.000398     0.000393     5.21e-06       0.0524       0.0678       0.0527       0.0518       0.0522       0.0667         0.07       0.0683       0.0852       0.0071\n",
      "     22   150     0.000368     0.000364     3.69e-06       0.0532       0.0653       0.0519       0.0559       0.0539       0.0638       0.0682        0.066       0.0758      0.00632\n",
      "     22   160       0.0004      0.00039     1.01e-05       0.0529       0.0676         0.05       0.0588       0.0544       0.0642       0.0738        0.069        0.129       0.0108\n",
      "     22   170     0.000609      0.00054      6.9e-05        0.062       0.0795       0.0602       0.0656       0.0629       0.0785       0.0815         0.08         0.34       0.0283\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     22     5     0.000333     0.000332     5.93e-07       0.0475       0.0624       0.0439       0.0546       0.0493        0.058       0.0703       0.0641       0.0196      0.00163\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              22 3147.516    0.005     0.000436     1.73e-05     0.000453       0.0542       0.0715       0.0525       0.0576        0.055       0.0691       0.0759       0.0725        0.139       0.0116\n",
      "! Validation         22 3147.516    0.005     0.000292     4.41e-07     0.000292       0.0451       0.0584       0.0434       0.0485       0.0459       0.0568       0.0616       0.0592         0.02      0.00167\n",
      "Wall time: 3147.5170090419706\n",
      "! Best model       22    0.000\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     23    10      0.00037     0.000353     1.67e-05       0.0514       0.0643       0.0478       0.0585       0.0531       0.0587       0.0743       0.0665        0.166       0.0138\n",
      "     23    20      0.00029      0.00029     2.89e-07       0.0459       0.0583       0.0475       0.0425        0.045        0.062         0.05        0.056       0.0206      0.00172\n",
      "     23    30     0.000488     0.000484     4.09e-06       0.0593       0.0753       0.0615       0.0549       0.0582       0.0792       0.0667        0.073       0.0751      0.00626\n",
      "     23    40     0.000725     0.000453     0.000272       0.0566       0.0729       0.0505       0.0687       0.0596       0.0651       0.0863       0.0757        0.676       0.0563\n",
      "     23    50     0.000409     0.000402     6.45e-06       0.0528       0.0686       0.0492       0.0602       0.0547       0.0637       0.0776       0.0706       0.0992      0.00827\n",
      "     23    60     0.000534      0.00053     3.31e-06       0.0636       0.0788       0.0629       0.0649       0.0639       0.0775       0.0815       0.0795       0.0667      0.00556\n",
      "     23    70     0.000935     0.000907     2.86e-05        0.081        0.103       0.0726       0.0979       0.0853       0.0936         0.12        0.107        0.213       0.0177\n",
      "     23    80     0.000579     0.000493     8.59e-05       0.0595        0.076       0.0579       0.0628       0.0604       0.0739       0.0801        0.077        0.376       0.0313\n",
      "     23    90      0.00029     0.000277     1.29e-05        0.044        0.057       0.0446       0.0428       0.0437       0.0585       0.0538       0.0561        0.146       0.0122\n",
      "     23   100     0.000436     0.000432     3.04e-06       0.0556       0.0712       0.0533       0.0601       0.0567       0.0654       0.0815       0.0734       0.0556      0.00463\n",
      "     23   110     0.000305     0.000293      1.2e-05       0.0458       0.0586       0.0462       0.0452       0.0457       0.0598       0.0562        0.058        0.142       0.0118\n",
      "     23   120      0.00038     0.000375     5.14e-06        0.049       0.0662       0.0522       0.0426       0.0474       0.0694       0.0595       0.0644       0.0832      0.00693\n",
      "     23   130     0.000566     0.000543     2.35e-05       0.0617       0.0797       0.0594       0.0664       0.0629       0.0789       0.0815       0.0802        0.198       0.0165\n",
      "     23   140     0.000644     0.000611     3.31e-05       0.0635       0.0846       0.0661       0.0582       0.0621       0.0877       0.0779       0.0828        0.236       0.0197\n",
      "     23   150     0.000463     0.000455      8.4e-06       0.0548        0.073       0.0533       0.0576       0.0555       0.0714       0.0762       0.0738        0.117      0.00978\n",
      "     23   160     0.000375     0.000375     4.85e-07       0.0492       0.0663       0.0473       0.0529       0.0501       0.0653       0.0683       0.0668       0.0266      0.00222\n",
      "     23   170     0.000398     0.000396     1.35e-06       0.0534       0.0681        0.051       0.0582       0.0546       0.0658       0.0726       0.0692       0.0459      0.00382\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     23     5     0.000323     0.000322     4.93e-07       0.0464       0.0614       0.0431       0.0532       0.0481       0.0571       0.0693       0.0632       0.0195      0.00162\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              23 3290.745    0.005     0.000447     2.66e-05     0.000474       0.0544       0.0724       0.0528       0.0575       0.0552       0.0701       0.0768       0.0734        0.165       0.0138\n",
      "! Validation         23 3290.745    0.005     0.000282     3.74e-07     0.000282       0.0442       0.0575       0.0428        0.047       0.0449        0.056       0.0603       0.0582       0.0192       0.0016\n",
      "Wall time: 3290.745600666967\n",
      "! Best model       23    0.000\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     24    10     0.000254     0.000251     3.21e-06       0.0445       0.0542        0.043       0.0474       0.0452       0.0518       0.0586       0.0552       0.0722      0.00602\n",
      "     24    20     0.000247     0.000244     2.99e-06       0.0397       0.0535        0.041       0.0371       0.0391        0.055       0.0505       0.0527       0.0658      0.00548\n",
      "     24    30     0.000282      0.00028     2.53e-06        0.044       0.0572       0.0407       0.0504       0.0456       0.0531       0.0647       0.0589       0.0627      0.00523\n",
      "     24    40     0.000417     0.000416     6.98e-07       0.0554       0.0698       0.0566       0.0532       0.0549       0.0688       0.0718       0.0703       0.0272      0.00226\n",
      "     24    50     0.000531     0.000471        6e-05       0.0535       0.0743       0.0513       0.0578       0.0545       0.0705       0.0813       0.0759        0.315       0.0263\n",
      "     24    60     0.000372     0.000371     8.74e-07         0.05       0.0659        0.044       0.0621       0.0531       0.0594       0.0773       0.0684       0.0315      0.00262\n",
      "     24    70     0.000202     0.000202     2.02e-07       0.0391       0.0486       0.0393       0.0386       0.0389       0.0502       0.0454       0.0478       0.0177      0.00147\n",
      "     24    80     0.000241     0.000227     1.45e-05       0.0406       0.0515       0.0381       0.0455       0.0418       0.0502        0.054       0.0521        0.155       0.0129\n",
      "     24    90     0.000322     0.000321     1.31e-06       0.0477       0.0613       0.0498       0.0437       0.0467        0.065       0.0531       0.0591       0.0442      0.00368\n",
      "     24   100     0.000306     0.000305     1.13e-06       0.0478       0.0598       0.0496       0.0443       0.0469       0.0624       0.0541       0.0583       0.0298      0.00249\n",
      "     24   110     0.000352      0.00035     2.54e-06       0.0489        0.064       0.0513       0.0443       0.0478       0.0669       0.0579       0.0624       0.0577      0.00481\n",
      "     24   120     0.000442     0.000442     5.04e-07       0.0566       0.0719       0.0586       0.0527       0.0556       0.0757       0.0637       0.0697       0.0264       0.0022\n",
      "     24   130     0.000244     0.000228     1.55e-05        0.041       0.0517       0.0363       0.0505       0.0434       0.0453       0.0626       0.0539        0.161       0.0134\n",
      "     24   140     0.000236     0.000236     2.99e-07       0.0419       0.0525       0.0403        0.045       0.0427       0.0513       0.0549       0.0531       0.0174      0.00145\n",
      "     24   150     0.000511     0.000506     5.94e-06       0.0588        0.077       0.0573       0.0618       0.0595        0.074       0.0825       0.0783       0.0972       0.0081\n",
      "     24   160      0.00029     0.000285     4.97e-06       0.0467       0.0578       0.0445       0.0511       0.0478        0.055       0.0629        0.059       0.0909      0.00758\n",
      "     24   170     0.000408     0.000405     2.74e-06       0.0504       0.0689       0.0508       0.0496       0.0502       0.0699       0.0668       0.0684       0.0635      0.00529\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     24     5     0.000298     0.000297     4.47e-07       0.0452        0.059       0.0423       0.0511       0.0467       0.0554       0.0656       0.0605       0.0194      0.00162\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              24 3439.128    0.005     0.000332     7.03e-06     0.000339        0.048       0.0624       0.0473       0.0495       0.0484       0.0616        0.064       0.0628       0.0838      0.00698\n",
      "! Validation         24 3439.128    0.005      0.00027     3.42e-07      0.00027       0.0434       0.0562       0.0419       0.0464       0.0442       0.0547       0.0591       0.0569       0.0186      0.00155\n",
      "Wall time: 3439.1292858750094\n",
      "! Best model       24    0.000\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     25    10     0.000523     0.000468     5.56e-05       0.0586        0.074       0.0593       0.0571       0.0582       0.0742       0.0736       0.0739        0.305       0.0254\n",
      "     25    20     0.000284      0.00028      4.7e-06       0.0428       0.0573       0.0435       0.0415       0.0425       0.0582       0.0554       0.0568        0.081      0.00675\n",
      "     25    30     0.000408     0.000398     9.68e-06       0.0517       0.0683       0.0513       0.0526        0.052       0.0671       0.0706       0.0689        0.122       0.0101\n",
      "     25    40     0.000698     0.000591     0.000107       0.0612       0.0832       0.0571       0.0694       0.0632       0.0797       0.0898       0.0847        0.424       0.0354\n",
      "     25    50     0.000406     0.000401      5.8e-06       0.0508       0.0685       0.0513       0.0496       0.0505       0.0696       0.0663       0.0679       0.0974      0.00812\n",
      "     25    60      0.00037     0.000369     1.44e-06        0.049       0.0657       0.0449       0.0572        0.051       0.0585       0.0782       0.0684       0.0463      0.00386\n",
      "     25    70     0.000395     0.000388     6.76e-06       0.0504       0.0674       0.0515       0.0481       0.0498       0.0675       0.0673       0.0674          0.1      0.00834\n",
      "     25    80     0.000643     0.000641     1.93e-06       0.0675       0.0866       0.0629       0.0768       0.0698       0.0833        0.093       0.0881       0.0491      0.00409\n",
      "     25    90     0.000265     0.000264     4.95e-07       0.0455       0.0556       0.0461       0.0441       0.0451       0.0563       0.0542       0.0553       0.0245      0.00204\n",
      "     25   100      0.00052     0.000449     7.12e-05       0.0567       0.0725       0.0541       0.0619        0.058       0.0701       0.0771       0.0736        0.345       0.0288\n",
      "     25   110     0.000322     0.000319     3.45e-06       0.0473       0.0611       0.0419       0.0581         0.05       0.0543       0.0728       0.0636       0.0722      0.00602\n",
      "     25   120     0.000376     0.000354     2.25e-05       0.0479       0.0644       0.0496       0.0446       0.0471       0.0676       0.0575       0.0625        0.194       0.0162\n",
      "     25   130     0.000537     0.000523     1.41e-05       0.0546       0.0783       0.0519       0.0601        0.056       0.0726       0.0886       0.0806        0.149       0.0124\n",
      "     25   140     0.000527     0.000495     3.19e-05       0.0605       0.0762         0.06       0.0615       0.0607       0.0765       0.0755        0.076        0.231       0.0192\n",
      "     25   150      0.00037     0.000365     5.64e-06       0.0523       0.0654       0.0503       0.0561       0.0532       0.0636       0.0688       0.0662        0.093      0.00775\n",
      "     25   160     0.000243     0.000241     2.34e-06       0.0434       0.0531       0.0432       0.0439       0.0435       0.0527       0.0539       0.0533       0.0553      0.00461\n",
      "     25   170     0.000289     0.000289     1.22e-07       0.0453       0.0582       0.0438       0.0483        0.046       0.0565       0.0613       0.0589         0.01     0.000836\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     25     5     0.000277     0.000276     4.81e-07        0.044       0.0569       0.0414       0.0491       0.0452       0.0539       0.0624       0.0582       0.0204       0.0017\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              25 3550.108    0.005     0.000386     1.66e-05     0.000402       0.0514       0.0672       0.0506        0.053       0.0518       0.0661       0.0694       0.0678        0.136       0.0113\n",
      "! Validation         25 3550.108    0.005     0.000256     3.45e-07     0.000257       0.0424       0.0548       0.0411       0.0449        0.043       0.0536       0.0571       0.0554       0.0184      0.00153\n",
      "Wall time: 3550.1089480830124\n",
      "! Best model       25    0.000\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     26    10     0.000325     0.000316     8.75e-06       0.0478       0.0609       0.0437       0.0559       0.0498       0.0547       0.0717       0.0632         0.12      0.00998\n",
      "     26    20     0.000315     0.000307     7.57e-06       0.0465         0.06       0.0464       0.0466       0.0465       0.0601       0.0599         0.06        0.112      0.00935\n",
      "     26    30     0.000283     0.000281      1.9e-06        0.045       0.0574       0.0405       0.0539       0.0472       0.0511       0.0681       0.0596       0.0502      0.00418\n",
      "     26    40     0.000392     0.000384     8.09e-06       0.0538        0.067       0.0557       0.0501       0.0529       0.0675       0.0662       0.0668        0.115      0.00961\n",
      "     26    50     0.000299     0.000287     1.19e-05       0.0472        0.058       0.0437       0.0542        0.049       0.0541        0.065       0.0596         0.14       0.0117\n",
      "     26    60     0.000521       0.0005     2.13e-05       0.0558       0.0765       0.0513       0.0646        0.058       0.0709       0.0868       0.0788        0.187       0.0156\n",
      "     26    70     0.000205     0.000193     1.24e-05       0.0386       0.0475       0.0375       0.0409       0.0392       0.0451       0.0519       0.0485        0.143       0.0119\n",
      "     26    80     0.000244     0.000243     6.18e-07       0.0398       0.0534       0.0368       0.0458       0.0413       0.0486       0.0618       0.0552       0.0254      0.00211\n",
      "     26    90     0.000402      0.00039     1.14e-05       0.0524       0.0676       0.0524       0.0526       0.0525       0.0656       0.0714       0.0685        0.131        0.011\n",
      "     26   100      0.00024     0.000238     1.52e-06       0.0409       0.0529       0.0401       0.0426       0.0413       0.0534       0.0518       0.0526       0.0499      0.00416\n",
      "     26   110     0.000266     0.000265     1.03e-06       0.0439       0.0557       0.0462       0.0394       0.0428       0.0575       0.0521       0.0548       0.0404      0.00337\n",
      "     26   120     0.000266     0.000264     2.32e-06       0.0435       0.0556       0.0455       0.0396       0.0426       0.0574       0.0518       0.0546       0.0621      0.00517\n",
      "     26   130     0.000297     0.000296     1.22e-06       0.0465       0.0589       0.0453        0.049       0.0472       0.0571       0.0624       0.0597       0.0372       0.0031\n",
      "     26   140     0.000355     0.000354     5.12e-07       0.0499       0.0644       0.0465       0.0568       0.0516       0.0616       0.0697       0.0656       0.0265      0.00221\n",
      "     26   150     0.000486      0.00048      5.8e-06       0.0554        0.075       0.0508       0.0646       0.0577       0.0671       0.0887       0.0779       0.0979      0.00816\n",
      "     26   160     0.000395     0.000344     5.11e-05       0.0508       0.0635       0.0532        0.046       0.0496       0.0654       0.0593       0.0624        0.293       0.0244\n",
      "     26   170     0.000311     0.000304     6.94e-06       0.0457       0.0597       0.0435       0.0502       0.0468       0.0548       0.0684       0.0616        0.104      0.00865\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     26     5     0.000267     0.000267     4.99e-07       0.0432       0.0559       0.0411       0.0474       0.0443       0.0537       0.0601       0.0569       0.0196      0.00163\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              26 3659.969    0.005     0.000319     9.94e-06     0.000329        0.047       0.0611       0.0462       0.0487       0.0474         0.06       0.0632       0.0616        0.101      0.00842\n",
      "! Validation         26 3659.969    0.005     0.000247     3.64e-07     0.000247       0.0418       0.0538       0.0406        0.044       0.0423       0.0527       0.0559       0.0543       0.0187      0.00156\n",
      "Wall time: 3659.970022708061\n",
      "! Best model       26    0.000\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     27    10      0.00042     0.000419     1.07e-06       0.0532         0.07       0.0566       0.0462       0.0514       0.0743       0.0604       0.0674       0.0411      0.00342\n",
      "     27    20     0.000244     0.000241     2.23e-06       0.0431       0.0532       0.0439       0.0415       0.0427        0.054       0.0514       0.0527       0.0558      0.00465\n",
      "     27    30     0.000248     0.000247     8.53e-07       0.0432       0.0538       0.0434       0.0427       0.0431        0.053       0.0552       0.0541       0.0336       0.0028\n",
      "     27    40     0.000404     0.000402     1.75e-06       0.0543       0.0686       0.0545       0.0538       0.0542       0.0668       0.0722       0.0695       0.0511      0.00426\n",
      "     27    50     0.000334     0.000333     2.75e-07       0.0516       0.0625       0.0502       0.0543       0.0523       0.0614       0.0646        0.063       0.0159      0.00132\n",
      "     27    60     0.000239     0.000238     1.05e-06       0.0408       0.0528       0.0387        0.045       0.0419       0.0485       0.0604       0.0545        0.038      0.00316\n",
      "     27    70     0.000424     0.000419     5.14e-06       0.0538       0.0701       0.0543       0.0527       0.0535       0.0708       0.0686       0.0697       0.0909      0.00757\n",
      "     27    80     0.000396     0.000379     1.74e-05       0.0526       0.0666       0.0495       0.0588       0.0542       0.0638       0.0718       0.0678        0.168        0.014\n",
      "     27    90     0.000363     0.000324     3.94e-05        0.046       0.0616       0.0422       0.0537       0.0479       0.0565       0.0707       0.0636        0.257       0.0214\n",
      "     27   100     0.000379     0.000357      2.2e-05       0.0524       0.0647       0.0522       0.0527       0.0524       0.0637       0.0666       0.0652        0.191       0.0159\n",
      "     27   110     0.000315     0.000312     2.86e-06       0.0471       0.0605       0.0483       0.0448       0.0466       0.0605       0.0604       0.0604       0.0671      0.00559\n",
      "     27   120     0.000275     0.000241     3.37e-05       0.0423       0.0532       0.0402       0.0466       0.0434       0.0514       0.0566        0.054        0.238       0.0198\n",
      "     27   130     0.000226      0.00022     6.05e-06       0.0392       0.0507       0.0388       0.0401       0.0394       0.0491       0.0537       0.0514       0.0984       0.0082\n",
      "     27   140     0.000504     0.000502     2.15e-06       0.0589       0.0767       0.0583       0.0599       0.0591       0.0773       0.0755       0.0764       0.0511      0.00426\n",
      "     27   150     0.000356     0.000354     2.35e-06       0.0503       0.0644       0.0493       0.0521       0.0507       0.0642       0.0647       0.0645       0.0589      0.00491\n",
      "     27   160     0.000406     0.000403     2.89e-06       0.0539       0.0687       0.0512       0.0593       0.0553       0.0669       0.0724       0.0696       0.0624       0.0052\n",
      "     27   170       0.0003     0.000298     1.35e-06       0.0444       0.0591       0.0447       0.0436       0.0442       0.0583       0.0606       0.0595       0.0384       0.0032\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     27     5     0.000272     0.000272     3.88e-07       0.0428       0.0564       0.0407        0.047       0.0438       0.0536       0.0617       0.0577       0.0184      0.00153\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              27 3772.146    0.005       0.0003     1.11e-05     0.000311       0.0456       0.0593       0.0453       0.0463       0.0458       0.0589       0.0599       0.0594        0.106      0.00884\n",
      "! Validation         27 3772.146    0.005     0.000242     2.97e-07     0.000242       0.0411       0.0532         0.04       0.0432       0.0416        0.052       0.0555       0.0538       0.0173      0.00145\n",
      "Wall time: 3772.1465022920165\n",
      "! Best model       27    0.000\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      O_f_mae     Si_f_mae  psavg_f_mae     O_f_rmse    Si_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     28    10     0.000241     0.000241     4.58e-07       0.0407       0.0531       0.0402       0.0417        0.041       0.0518       0.0555       0.0537       0.0219      0.00183\n",
      "     28    20     0.000227     0.000221     6.04e-06       0.0399       0.0508       0.0381       0.0435       0.0408       0.0493       0.0538       0.0515       0.0984       0.0082\n",
      "     28    30     0.000216     0.000212      4.4e-06       0.0383       0.0498       0.0364       0.0421       0.0392       0.0481       0.0531       0.0506       0.0834      0.00695\n",
      "     28    40     0.000359     0.000358     5.95e-07       0.0482       0.0648        0.048       0.0486       0.0483       0.0642        0.066       0.0651       0.0204       0.0017\n",
      "     28    50     0.000319     0.000316     3.08e-06       0.0472       0.0608       0.0491       0.0434       0.0463       0.0635       0.0551       0.0593       0.0629      0.00524\n"
     ]
    }
   ],
   "source": [
    "### Here we use \"!\" to run a command line argument. What this means is that the jupyter notebook runs this command like\n",
    "### You entered it directly into your terminal\n",
    "\n",
    "### IMPORTANT: This bit of code will remove the previous results. Please comment this out if you don't want that.\n",
    "### If it says \"rm: cannot remove 'results': No such file or directory\", just ignore it. That means you haven't\n",
    "### trained anything yet.\n",
    "import os\n",
    "os.system(\"rm -r results\")\n",
    "############################################################################################################\n",
    "\n",
    "!nequip-train train.yaml\n",
    "print(\"I am done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f01e33-75c3-409c-8347-25bb0099c584",
   "metadata": {},
   "source": [
    "### Now lets deploy the model. What this means is that we will transform the model into something that can be used the the atomic simulation environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d0caf0-8730-421d-9873-f645eb7de731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system(\"mkdir deployed_model\")\n",
    "!nequip-deploy build --train-dir results/SiO_training/SiO-training-run deployed_model/siomodel.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfa30cf-6cfe-4294-a4c3-e1a5a2c4657a",
   "metadata": {},
   "source": [
    "### This saves a version of the model to the folder 'deployed_model/siomodel.pth'. We can now use this model to do some testing with ASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff773f8-6939-48ab-a04d-7f8efda4ad44",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lets first read the testing structures that we saved earlier\n",
    "from ase.io import read\n",
    "test_structures = read(\"test.extxyz\", index=':', format='extxyz')\n",
    "\n",
    "\n",
    "### Since we already saved the energies of these structures, we can simply run structure.get_potential_energy() \n",
    "### to get the energy that we saved \n",
    "### NOTE: At this stage we have not yet loaded in the ML Potential\n",
    "print(\"DFT Energy of structure 11 is\", test_structures[10].get_potential_energy(), \"eV\")\n",
    "\n",
    "\n",
    "\n",
    "### Now lets see what the model predicts. For this, we can make a copy of the atoms object for structure 11 and assign the \"calculator\"\n",
    "### to the ML Potential. This will override the previous \"calculator\" which just returned the previously calculated energies. \n",
    "### The NEQUIP model will instead fully recalculate the energies\n",
    "from nequip.ase.nequip_calculator import nequip_calculator\n",
    "test_structure = test_structures[10].copy()\n",
    "\n",
    "## Note how we use the model we just saved\n",
    "test_structure.calc = nequip_calculator(\"deployed_model/siomodel.pth\")\n",
    "print(\"ML Potential energy of structure 11 is\", test_structure.get_potential_energy(), \"eV\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26de5f72-eae4-4866-b870-c73ad9a7d796",
   "metadata": {},
   "source": [
    "### Lets now make a graph of the energies predicted by the ML algorithm and the true energies from DFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068c260a-c976-4425-95f9-b87e00abf2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This library just makes a progress bar so that we can see how long the for loop will take. Feel free to remove \n",
    "### it if you don't mind not having a progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "### First, we make a list of the energies from DFT\n",
    "dft_energies = []\n",
    "for i in range(len(test_structures)):\n",
    "    dft_energy = test_structures[i].get_potential_energy()\n",
    "    dft_energies.append(dft_energy)\n",
    "\n",
    "### Next, we make a list of energies calculated by the ML Potential\n",
    "## For simplicity, we define the calculator in the beginning\n",
    "ml_calc = nequip_calculator(\"deployed_model/siomodel.pth\")\n",
    "ml_energies = []\n",
    "for i in tqdm(range(len(test_structures))):\n",
    "    test_structure = test_structures[i].copy()\n",
    "    test_structure.calc = ml_calc\n",
    "    ml_energy = test_structure.get_potential_energy()\n",
    "    ml_energies.append(ml_energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f59dc8e-1e14-438d-87e8-f18db5d06b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now we can go ahead and plot the results\n",
    "plt.scatter(dft_energies, ml_energies)\n",
    "plt.xlabel(\"DFT Energy (eV)\")\n",
    "plt.ylabel(\"ML Energy (eV)\")\n",
    "\n",
    "### Lets also make a line that represents perfect agreement (that means the DFT energy exactly matches the ML Energy)\n",
    "x = np.linspace(np.amin(dft_energies), np.amax(dft_energies), 100)\n",
    "plt.plot(x, x, c='black', linestyle='--')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868ed916-f6a0-4239-bac2-db1771d8af26",
   "metadata": {},
   "source": [
    "## We can see that the ML energy matches the DFT energy quite well. This is good and the results can be improved either by including more training data, or by increasing the number of epochs or making the model larger. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54c0245-3b01-49b7-a467-b5422a3d2fdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
